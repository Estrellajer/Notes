--- 
 title: 持续学习论文阅读
 date: 2024-07-05 14:53:42 
 updated: 2024-07-13
 mathjax: true
tags:
  - 持续学习
  - 论文阅读
---

## 综述
### Continual Learning Scenarios

#### Instance-Incremental Learning (IIL)
- **定义**: 所有训练样本属于相同任务，以批次到达。
![](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407141840849.png)

#### Domain-Incremental Learning (DIL)
- **定义**: 任务具有相同的数据标签空间但不同的输入分布。任务标识不是必须的。
![image.png](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407141840293.png)


#### Task-Incremental Learning (TIL)
- **定义**: 任务具有不相交的数据标签空间。在训练和测试中提供任务标识。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；提供任务标识

#### Class-Incremental Learning (CIL)
- **定义**: 任务具有不相交的数据标签空间。仅在训练中提供任务标识。
![image.png](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407141840752.png)

#### Task-Free Continual Learning (TFCL)
- **定义**: 任务具有不相交的数据标签空间。在训练和测试中均不提供任务标识。
![](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407141841203.png)

#### Online Continual Learning (OCL)
- **定义**: 任务具有不相交的数据标签空间。每个任务的训练样本以一次性数据流形式到达。
![image.png](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407141842687.png)


#### Blurred Boundary Continual Learning (BBCL)
- **定义**: 任务边界模糊，具有不同但重叠的数据标签空间。
![image.png](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407141842009.png)


#### Continual Pre-training (CPT)
- **定义**: 预训练数据按顺序到达。目标是改进知识传递到下游任务。
![image.png](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407141842218.png)


#### 其他学习场景
除非特别说明，每个任务通常假定有足够数量的标记训练样本，即监督连续学习。根据每个 Dt 中提供的 Xt 和 Yt，连续学习进一步扩展到零样本学习、少样本学习、半监督学习、开放世界学习（识别未知类别并整合其标签）和无监督/自监督学习场景。此外，还考虑并纳入了其他实际挑战，例如多标签、噪声标签、层次粒度和子群体、任务相似性的混合、长尾分布、域对齐、域迁移、随时推理、新类别发现、多模态等。一些最新的工作集中在这些场景的各种组合上，以更好地模拟现实世界的复杂性。

### 连续学习的性能评估

通常，连续学习的性能可以从三个方面进行评估：已经学习任务的总体性能、旧任务的记忆稳定性和新任务的学习可塑性。以下是常见的评估指标，使用分类作为示例。

#### 总体性能
总体性能通常通过平均准确率 (AA) 和平均增量准确率 (AIA) 来评估。

设 $a_{k,j} \in [0, 1]$ 表示在增量学习第 $k$ 个任务后，在第 $j$ 个任务的测试集上评估的分类准确率 ($j \leq k$)。计算 $a_{k,j}$ 的输出空间包括 $Y_j$ 或 $\bigcup_{i=1}^{k} Y_i$，分别对应多头评估 (如 TIL) 或单头评估 (如 CIL)。

第 \( k \) 个任务的两个指标定义如下：
$$
AA_k = \frac{1}{k} \sum_{j=1}^{k} a_{k,j}
$$
$$
AIA_k = \frac{1}{k} \sum_{i=1}^{k} AA_i
$$
其中，AA 代表当前时刻的总体性能，AIA 进一步反映了历史性能。

#### 记忆稳定性
记忆稳定性可以通过遗忘度量 (FM) 和向后传递 (BWT) 来评估。

对于前者，任务的遗忘通过其过去的最大性能与当前性能的差值计算：
$$
f_{j,k} = \max_{i \in \{1,...,k-1\}} (a_{i,j} - a_{k,j}), \forall j < k
$$
第 \( k \) 个任务的 FM 是所有旧任务的平均遗忘：
$$
FM_k = \frac{1}{k-1} \sum_{j=1}^{k-1} f_{j,k}
$$
对于后者，BWT 评估学习第 \( k \) 个任务对所有旧任务的平均影响：
$$
BWT_k = \frac{1}{k-1} \sum_{j=1}^{k-1} (a_{k,j} - a_{j,j})
$$
其中，遗忘通常反映为负的 BWT。

#### 学习可塑性
学习可塑性可以通过不易学习度量 (IM) 和向前传递 (FWT) 来评估。

IM 定义为模型学习新任务的能力不足，通过任务的联合训练性能与持续学习性能之间的差异计算：
$$
IM_k = a^*_k - a_{k,k}
$$
其中，$a^*_k$ 是随机初始化的参考模型在联合训练 $\bigcup_{j=1}^{k} D_j$ 时第 $k$ 个任务的分类准确率。相比之下，FWT 评估所有旧任务对当前第 $k$ 个任务的平均影响：
$$
FWT_k = \frac{1}{k-1} \sum_{j=2}^{k} (a_{j,j} - \tilde{a}_j)
$$
其中，$\tilde{a}_j$ 是随机初始化的参考模型在第 $j$ 个任务的 $D_j$ 上训练的分类准确率。

#### 其他评估指标
请注意，$a_{k,j}$ 可以根据任务类型适应其他形式，例如对象检测的平均精度 (AP)、语义分割的交并比 (IoU)、图像生成的 Frechet Inception Distance (FID)、强化学习的标准化奖励等，并应相应调整上述评估指标。此外，还有许多其他有用的指标，例如表示遗忘的线性探针、Hessian 矩阵的最大特征值用于损失景观的平坦性、任何时候推理的准确率曲线下面积、存储和计算的开销用于资源效率等。

### 理论基础

在本节中，我们总结了关于连续学习的理论工作，涉及稳定性-可塑性权衡和可推广性分析，并将其与各种连续学习方法的动机联系起来。

#### 稳定性-可塑性权衡

根据第2.1节的基本公式，我们考虑一个通用的连续学习设置，其中具有参数 $\theta \in \mathbb{R}^{|\theta|}$ 的神经网络需要学习 $k$ 个增量任务。假设每个任务的训练集和测试集遵循相同的分布 $D_t, t = 1, ..., k$，其中训练集 $D_t = \{X_t, Y_t\} = \{(x_{t,n}, y_{t,n})\}_{n=1}^{N_t}$ 包含 $N_t$ 对数据标签对。目标是学习一个概率模型 $p(D_{1:k}|\theta) = \prod_{t=1}^{k} p(D_t|\theta)$（在条件独立假设下），该模型能够在所有任务上表现良好，记为 $D_{1:k} := \{D_1, ..., D_k\}$。

判别模型的任务相关性能可以表示为 $\log p(D_t|\theta) = \sum_{n=1}^{N_t} \log p_\theta(y_{t,n}|x_{t,n})$。连续学习的核心挑战通常来自学习的顺序性质：在从 $D_k$ 学习第 $k$ 个任务时，旧的训练集 $\{D_1, ..., D_{k-1}\}$ 是不可访问的。因此，关键但困难的是在捕获旧任务和新任务的分布时保持平衡，即确保适当的稳定性-可塑性权衡，其中过度的学习可塑性或记忆稳定性可能会大大妨碍对方（见图2，a，b）。

一个直接的想法是通过存储一些旧的训练样本或训练生成模型来近似和恢复旧的数据分布，这在第4.2节中称为基于重放的方法。根据监督学习的学习理论[154]，通过重放更多近似其分布的旧训练样本来提高旧任务的性能，但这会导致潜在的隐私问题和资源开销的线性增长。生成模型的使用也受到额外资源开销、其自身的灾难性遗忘和表达能力的限制。

另一种选择是通过在贝叶斯框架中公式化连续学习来传播旧数据分布。在网络参数的先验 $p(\theta)$ 基础上，观察第 $k$ 个任务后的后验可以通过贝叶斯规则计算：
$$
p(\theta|D_{1:k}) \propto p(\theta) \prod_{t=1}^{k} p(D_t|\theta) \propto p(\theta|D_{1:k-1})p(D_k|\theta),
$$
其中，第 $k-1$ 个任务的后验 $p(\theta|D_{1:k-1})$ 成为第 $k$ 个任务的先验，因此仅使用当前训练集 $D_k$ 就可以计算新的后验 $p(\theta|D_{1:k})$。

然而，由于后验通常是不可解的（除非在非常特殊的情况下），常见的选择是用 $q_{k-1}(\theta) \approx p(\theta|D_{1:k-1})$ 来近似，同样对于 $q_k(\theta) \approx p(\theta|D_{1:k})$。接下来，我们将介绍两种广泛使用的近似策略：

第一种是在线拉普拉斯近似，它使用局部梯度信息将 $p(\theta|D_{1:k-1})$ 近似为多元高斯[177]，[202]，[222]，[371]，[441]。具体来说，我们可以用 $\phi_{k-1}$ 参数化 $q_{k-1}(\theta)$ 并通过在 $p(\theta|D_{1:k-1})$ 的模态 $\mu_{k-1} \in \mathbb{R}^{|\theta|}$ 周围执行二阶泰勒展开构建一个近似高斯后验 $q_{k-1}(\theta) := q(\theta; \phi_{k-1}) = \mathcal{N}(\theta; \mu_{k-1}, \Lambda_{k-1}^{-1})$，其中 $\Lambda_{k-1}$ 表示精度矩阵，$\phi_{k-1} = \{\mu_{k-1}, \Lambda_{k-1}\}$，同样对于 $q(\theta; \phi_k), \mu_k 和 \Lambda_k$。

根据公式(8)，学习当前第 $k$ 个任务的后验模态可以计算为：
$$
\mu_k = \arg \max_\theta \log p(\theta|D_{1:k}) \approx \arg \max_\theta \log p(D_k|\theta) + \log q(\theta; \phi_{k-1}) = \arg \max_\theta \log p(D_k|\theta) - \frac{1}{2} (\theta - \mu_{k-1})^\top \Lambda_{k-1}(\theta - \mu_{k-1}),
$$
该更新递归自 $\mu_{k-1}$ 和 $\Lambda_{k-1}$。同时，$\Lambda_k$ 从 $\Lambda_{k-1}$ 递归更新：
$$
\Lambda_k = -\nabla^2_\theta \log p(\theta|D_{1:k})|_{\theta=\mu_k} \approx -\nabla^2_\theta \log p(D_k|\theta)|_{\theta=\mu_k} + \Lambda_{k-1},
$$
其中右侧的第一项是负对数似然的海森矩阵，记为 $H(D_k, \mu_k)$。实际上，由于 $\mathbb{R}^{|\theta|}$ 的高维性，$H(D_k, \mu_k)$ 通常计算效率低，并且没有保证近似的 $\Lambda_k$ 对于高斯假设是半正定的。为了解决这些问题，海森矩阵通常用费舍尔信息矩阵（FIM）来近似：
$$
F_k = \mathbb{E}[\nabla_\theta \log p(D_k|\theta) \nabla_\theta \log p(D_k|\theta)^\top]|_{\theta=\mu_k} \approx H(D_k, \mu_k)。
$$
为了便于计算，FIM 可以进一步简化为对角近似[177]，[222] 或克罗内克分解近似[293]，[371]。然后，公式(9) 通过保存旧模型 $\mu_{k-1}$ 的冻结副本来实现，以规避参数变化，称为第4.1节中的基于正则化的方法。这里我们以 EWC [222] 为例，给出其损失函数：
$$
L_{EWC}(\theta) = \ell_k(\theta) + \frac{\lambda}{2} (\theta - \mu_{k-1})^\top \hat{F}_{1:k-1}(\theta - \mu_{k-1}),
$$
其中 $\ell_k$ 表示特定任务的损失，FIM $\hat{F}_{1:k-1} = \sum_{t=1}^{k-1} \text{diag}(F_t)$ 使用每个 $F_t$ 的对角近似 $\text{diag}(\cdot)$，$\lambda$ 是控制正则化强度的超参数。

第二种是在线变分推理（VI）[7]，[91]，[206]，[235]，[248]，[280]，[318]，[378]，[410]。有很多不同的方法来实现这一点，其中一个代表性的方法是最小化当前第 $k$ 个任务在满足 $p(\theta|D_{1:k}) \in Q$ 的家族 Q 上的以下 KL 散度：
$$
q_k(\theta) = \arg \min_{q \in Q} KL(q(\theta) \parallel \frac{1}{Z_k} q_{k-1}(\theta)p(D_k|\theta)),
$$
其中 $Z_k$ 是 $q_{k-1}(\theta)p(D_k|\theta)$ 的归一化常数。实际上，上述最小化可以通过使用附加的蒙特卡罗近似来实现，并指定 $q_k(\theta) := q(\theta; \phi_k) = \mathcal{N}(\theta; \mu_k, \Lambda_k^{-1})$ 为多元高斯。这里我们以 VCL [318] 为例，它最小化以下目标（即最大化其负值）：
$$
L_{VCL}(q_k(\theta)) = \mathbb{E}_{q_k(\theta)}(\ell_k(\theta)) + KL(q_k(\theta) \parallel q_{k-1}(\theta))，
$$
其中 KL 散度可以以闭式形式计算，并作为隐含的正则化项。特别地，尽管公式(12) 和公式(14) 的损失函数形式相似，前者是基于一组确定参数 $\theta$ 的局部近似，而后者是通过从变分分布 $q_k(\theta)$ 采样计算的。这归因于两种近似策略之间的基本差异[318]，[420]，在特定设置中表现略有不同。

除了参数空间，顺序贝叶斯推理的思想也适用于函数空间[326]，[378]，[417]，这往往能够在更新参数时提供更多的灵活性。此外，还有许多 VI 的其他扩展，例如通过变分自回归高斯过程（VAR-GPs）改善后验更新[206]，构建任务特定参数[7]，[232]，[244]，[280]，以及适应非平稳数据流[235]。

本质上，无论是重放还是正则化，连续学习的约束最终都体现在梯度方向上。因此，一些最新的工作直接操纵基于梯度的优化过程，归类为第4.3节中的基于优化的方法。具体来说，当在内存缓冲区中保留任务 $t$ 的一些旧训练样本 $M_t$ 时，鼓励新训练样本的梯度方向与 $M_t$ 的梯度方向保持接近[66]，[281]，[411]。这被公式化为
$$
\langle\nabla_\theta L_k(\theta; D_k), \nabla_\theta L_k(\theta; M_t)\rangle \geq 0 \quad \text{for} \; t \in \{1, ..., k-1\},
$$
以实质上强制旧任务的损失不增加，即 $L_k(\theta; M_t) \leq L_k(\theta_{k-1}; M_t)$，其中 $\theta_{k-1}$ 是学习第 $k-1$ 个任务结束时的网络参数。

或者，梯度投影也可以在不存储旧训练样本的情况下进行[65]，[115]，[146]，[202]，[227]，[258]，[266]，[333]，[380]，[448]，[496]。这里我们以 NCL [202] 为例，它在在线拉普拉斯近似中使用 $\mu_{k-1}$ 和 $\Lambda_{k-1}$ 操作梯度方向。如公式(15)所示，NCL 通过在以 $\theta$ 为中心的半径 $r$ 区域内的距离度量 $d(\theta, \theta + \delta) = \sqrt{\delta^\top \Lambda_{k-1} \delta / 2}$ 进行最小化任务特定损失 $\ell_k(\theta)$ 来执行连续学习，该距离度量考虑了先验的曲率通过其精度矩阵 $\Lambda_{k-1}$：
$$
\delta^* = \arg \min_\delta \ell_k(\theta + \delta) \approx \arg \min_\delta \ell_k(\theta) + \nabla_\theta \ell_k(\theta)^\
$$
### 可推广性分析

当前的理论工作主要在增量任务的训练集上进行，假设它们的测试集遵循相似的分布，并且候选解决方案具有相似的可推广性。然而，由于学习多个任务的目标通常是高度非凸的，在每个训练集上表现相似但在测试集上具有显著不同可推广性的局部最优解很多[313], [443]。对于连续学习，理想的解决方案不仅需要从训练集到测试集的任务内可推广性，还需要适应其分布增量变化的任务间可推广性。

图2显示了连续学习的关键因素分析。a和b中，连续学习需要在学习可塑性和记忆稳定性之间保持适当的平衡，其中任一方面的过剩都会影响总体性能（改编自[443]）。c和d中，当收敛的损失景观更平坦且观察到的数据分布更相似时，适当平衡的解决方案可以更好地推广到任务序列（改编自[443]）。e中，参数空间的结构决定了找到理想解决方案的复杂性和可能性（改编自[225]）。黄色区域表示单个任务共享的可行参数空间，随着更多增量任务的引入，该空间往往变窄且不规则。

这里提供一个任务特定损失 \(\ell_t(\theta; D_t)\) 及其经验最优解 \(\theta^*_t = \arg \min_\theta \ell_t(\theta; D_t)\) 的概念演示。当任务 \(i\) 需要适应另一个任务 \(j\) 时，可以通过在 \(\theta^*_i\) 周围对 \(\ell_i(\theta; D_i)\) 进行二阶泰勒展开来估计其性能的最大牺牲：
$$
\ell_i(\theta^*_j; D_i) \approx \ell_i(\theta^*_i; D_i) + (\theta^*_j - \theta^*_i)^\top \nabla_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i} + \frac{1}{2} (\theta^*_j - \theta^*_i)^\top \nabla^2_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i} (\theta^*_j - \theta^*_i) \approx \ell_i(\theta^*_i; D_i) + \frac{1}{2} \Delta \theta^\top \nabla^2_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i} \Delta \theta,
$$
其中 \(\Delta \theta := \theta^*_j - \theta^*_i\) 并且 \(\nabla_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i} \approx 0\)。那么，任务 \(i\) 的性能退化上限为：
$$
\ell_i(\theta^*_j; D_i) - \ell_i(\theta^*_i; D_i) \leq \frac{1}{2} \lambda_{\max i} \|\Delta \theta\|^2,
$$
其中 \(\lambda_{\max i}\) 是海森矩阵 \(\nabla^2_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i}\) 的最大特征值。注意，任务 \(i\) 和 \(j\) 的顺序可以是任意的，即公式(20) 说明了前向和后向效果。因此，经验最优解 \(\theta^*_i\) 对参数变化的鲁棒性与 \(\lambda_{\max i}\) 密切相关，这已经成为描述损失景观平坦性的常用指标[163]，[212]，[313]。

直观地，收敛到具有更平坦损失景观的局部极小值对适度的参数变化不太敏感，因此有利于旧任务和新任务（见图2，c）。为了找到这样的平坦极小值，连续学习中有三种广泛使用的策略。

第一种来源于其定义，即平坦性指标。具体来说，可以将 \(\ell_t(\theta; D_t)\) 的最小化替换为鲁棒的任务特定损失 \(\ell_{tb}(\theta; D_t) := \max_{\|\delta\|\leq b} \ell_t(\theta + \delta; D_t)\)，因此获得的解不仅在特定点而且在其邻域内具有“半径” \(b\) 的低误差。然而，由于 \(\theta\) 的高维性，\(\ell_{tb}(\theta; D_t)\) 的计算无法覆盖所有可能的 \(\delta\)，只能覆盖少数方向[387]，类似于公式(19)中计算海森矩阵的复杂性问题。替代方法包括使用海森矩阵的近似[90]，[313] 或仅沿前向和后向参数变化的轨迹计算 \(\delta\)[58]，[174]，[183]，[300]，[312]。

第二种是在模式连通性的限制下，通过构建集成模型来操作损失景观，即沿低误差路径在参数或函数空间中集成多个极小值，因为连接它们可以确保该路径上的平坦性[60]，[131]，[183]，[312]，[443]，[462]。这两种策略与基于优化的方法密切相关。

第三种是获得分布良好的表示，这些表示在函数空间中的分布差异上更具鲁棒性，例如通过使用预训练[168]，[300]，[360]，更宽的网络架构[309]，[359]，[360] 和自监督学习[57]，[168]，[286]，[343]。观察到对大规模预训练和自监督学习的高度关注，我们将这一方向归为第4.4节中的基于表示的方法。

还有许多其他因素对连续学习的性能很重要。如公式(20)所示，性能退化的上限还取决于每个任务的经验最优解 \(\theta^*_t = \arg \min_\theta \ell_t(\theta; D_t)\) 的差异，即任务分布的差异（见图2，d），这一点通过遗忘-泛化权衡的理论分析[358]和泛化误差的PAC-Bayes界[338]，[443]进一步验证。因此，如何利用任务相似性与连续学习的性能直接相关。每个任务的泛化误差可以通过协同任务提高，但在竞争任务中恶化[361]，其中在共享解决方案中同等学习所有任务往往会在性能上妥协每个任务[361]，[443]。

另一方面，当模型参数不被所有任务共享（例如，使用多头输出层）时，任务相似性对连续学习的影响将会复杂。一些使用神经切线核（NTK）的理论研究[33]，[95]，[207]，[243]表明，任务相似性的增加可能导致更多的遗忘。由于输出头是独立的，因此更难区分两个相似的解决方案[207]，[243]。具体来说，在NTK体制下，从第 \(t\) 个任务到第 \(k\) 个任务，旧任务的遗忘有界于：
$$
\|p(D_k |\theta^*_k) - p(D_k|\theta^*_t )\|_F^2 \leq \sigma^2_{t,|rep|} + \sum_{i=t+1}^{k} \left( \Theta_{t \to S(i,|rep|)} \right)^2 \|\text{RES}_i\|_2^2。
$$
\(\Theta_{t \to k}\) 是一个对角矩阵，其中每个对角元素 \(\cos(\theta_{t,k})_r\) 是特征空间中第 \(t\) 和第 \(k\) 个任务之间第 \(r\) 个主角的余弦。 \(\sigma_{t,\cdot}\) 是第 \(t\) 个任务的 \(\cdot\) 的奇异值。 \(\text{RES}_i\) 是需要学习的旋转残差， \(S(i, \cdot)\) 表示直到第 \(i\) 个任务的残差子空间的阶数。 \(|rep|\) 是重放数据的样本数。任务相似性的复杂影响表明了模型架构在协调任务共享和任务特定组件方面的重要性。

此外，找到连续学习理想解决方案的复杂性在很大程度上取决于参数空间的结构。用共享解决方案学习所有增量任务等同于在受限参数空间中学习每个新任务，以防止所有旧任务的性能退化。这样的经典连续学习问题已被证明在一般情况下是NP难的[225]，因为随着更多任务的引入，可行参数空间往

往变得狭窄且不规则，因此难以识别（见图2，e）。这一具有挑战性的问题可以通过重放代表性的旧训练样本[225]，将可行参数空间限制在超矩形[459]，或使用多个连续学习模型替代单个参数空间的模型架构（例如，使用多个连续学习模型）来缓解[96]，[361]，[443]。

为了协调连续学习中的重要因素，最近的工作提出了一种类似的学习和遗忘的广义界形式。例如，在假设所有任务共享具有统一收敛性的全局极小化器（即公式(20)中对于 \(\forall t = 1, ···, k\)，\(\lambda_{\max i} = \lambda\)）下的理想连续学习者[337]，其泛化界为：
$$
c^*_t \leq \mathbb{E}_{D_t \sim D_t} \ell_t(\theta; D_t) \leq c^*_t + \zeta(N_t, \delta), \forall t = 1, ···, k,
$$
其中 \(c^*_t = \ell_t(\theta^*_t; D_t)\) 是第 \(t\) 个任务的最小损失，\(\theta\) 是通过经验风险最小化连续学习 \(1 : k\) 任务的全局解。 \(\zeta = O\left( \frac{\lambda B \sqrt{\log(N_t) \log(|\theta|k/\delta)}}{2\sqrt{N_t}} \right)，并且 \|\theta\|_2 \leq B\)。

考虑到许多不同任务的共享参数空间可能是一个空集（见图2，e），即 \(\bigcup_{t=1}^{k} \theta_t = \emptyset\)，泛化界通过假设 \(K\) 个参数空间（通常 \(K \geq 1\)）来捕获所有任务而进一步细化[442]，[443]。对于新旧任务的泛化误差：
$$
\mathbb{E}_{D_t \sim D_t} \ell_t(\theta; D_t) \leq c^*_t + R\left(\sum_{i=1}^{t-1} \ell^b_i\right) + \sum_{i=1}^{t-1} \text{Div}(D_i, D_t) + \zeta\left(\sum_{i=1}^{t-1} N_i, K/\delta\right),
$$
$$
\sum_{i=1}^{t-1} \mathbb{E}_{D_i \sim D_i} \ell_i(\theta; D_i) \leq \sum_{i=1}^{t-1} c^*_i + R(\ell^b) + \sum_{i=1}^{t-1} \text{Div}(D_t, D_i) + \zeta(N_t, K/\delta)，
$$
其中 \(R(·)\) 和 \(\text{Div}\) 分别是损失平坦性和任务差异的函数。 \(\delta, \theta 和 c^*_t\) 的定义与公式(22) 相同。

这些理论工作表明，理想的连续学习解决方案应提供适当的稳定性-可塑性权衡和足够的任务内/任务间可推广性，这激发了下一节中详细介绍的各种代表性方法。

以下是调整了标题级数的内容，使其与前面的内容对应：

### 方法

在本节中，我们详细介绍了代表性连续学习方法的分类（见图3和图1，c），深入分析了它们的主要动机、典型实现和经验特性。

#### 基于正则化的方法

这一方向的特点是通过添加显式的正则化项来平衡旧任务和新任务，通常需要存储旧模型的冻结副本以供参考（见图4）。根据正则化的目标，这些方法可以分为两个子方向。

##### 权重正则化

权重正则化通过选择性地正则化网络参数的变化来实现。一个典型的实现是在损失函数中添加二次惩罚，根据每个参数对执行旧任务的贡献或“重要性”来惩罚其变化，例如公式(12)，这种形式最初源自贝叶斯框架下后验的在线拉普拉斯近似。重要性可以通过费舍尔信息矩阵（FIM）计算，如EWC [222]和一些更先进的版本[371]，[382]。同时，许多工作致力于设计更好的重要性度量。SI [497] 在线近似每个参数的重要性，通过其对总损失变化的贡献和整个训练轨迹上的更新长度来计算。MAS [12] 基于对参数变化的预测结果敏感性累积重要性度量，这既是在线的也是无监督的。RWalk [63] 结合了SI [497] 和 EWC [222] 的正则化项以整合它们的优势。有趣的是，这些重要性度量被证明都相当于FIM的近似[34]，尽管它们的动机不同。

图4. 基于正则化的方法。该方向的特点是通过添加显式的正则化项来模拟旧模型的参数（权重正则化）或行为（功能正则化）。

还有一些工作改进了二次惩罚的实现。由于EWC中FIM的对角近似可能会丢失旧任务的信息，R-EWC [272] 对参数空间进行了因子化旋转以对角化FIM。XK-FAC [239] 在近似FIM时进一步考虑了样本间的关系，以更好地适应批量归一化。观察到参数变化对旧任务的不对称影响，ALASSO [329] 设计了一个不对称的二次惩罚，其中一侧被高估。与在旧模型约束下学习每个任务相比，这通常会加剧不易学习性，通过分别获取新任务解决方案并使用旧模型重新归一化的扩展-重新归一化过程显示出更好的稳定性-可塑性权衡[240]，[245]，[258]，[382]，[441]。

IMM [245] 是一个早期尝试，通过增量匹配旧任务和新任务后验分布的时刻，即它们解决方案的加权平均。ResCL [240] 扩展了这一思想，具有可学习的组合系数。P&C [382] 通过额外的网络单独学习每个任务，然后使用EWC的广义版本将其回流到旧模型中。AFEC [441] 引入了遗忘率，以消除公式(8)中原始后验 \(p(\theta|D_{1:k-1})\) 的潜在负迁移，并推导出二次项以惩罚网络参数 \(\theta\) 与旧任务和新任务解决方案的差异。为了可靠地平均旧任务和新任务解决方案，通过在低误差路径上约束它们来构建线性连接器[258]。

其他针对网络本身的正则化形式也属于这一子方向。如前所述，后验分布的在线变分推理可以作为参数变化的隐式正则化，例如 VCL [318], [410], NVCL [420], CLAW [7], GVCL [280], KCL [91] 和 VAR-GPs [206]。而不是巩固参数，NPC [325] 估计每个神经元的重要性，并选择性地降低其学习率。UCL [9] 和 AGS-CL [196] 冻结连接重要神经元的参数，相当于权重正则化的硬版本。

##### 功能正则化

功能正则化针对预测函数的中间或最终输出。该策略通常采用先前学习的模型作为教师，当前训练的模型作为学生，同时实施知识蒸馏（KD）[141] 以减轻灾难性遗忘。理想情况下，KD 的目标应该是所有旧训练样本，但在连续学习中这是不可用的。替代方案可以是新训练样本[93]，[181]，[255]，[362]，一小部分旧训练样本[53]，[102]，[165]，[365]，外部未标记数据[241]，生成数据[464]，[500] 等，存在不同程度的分布偏移。

作为开创性工作，LwF [255] 和 LwF.MC [365] 学习新训练样本，同时使用其来自旧任务输出头的预测来计算蒸馏损失。LwM [93] 利用新训练样本的注意力图进行 KD。EBLL [362] 学习任务特定的自动编码器并防止特征重构的变化。GD [241] 进一步从野外大量未标记数据中蒸馏知识。当旧训练样本被忠实恢复时，功能正则化的潜力可以得到极大释放。因此，功能正则化通常与重放一些旧训练样本结合使用，如 iCaRL [365]，EEIL [53]，LUCIR [165]，PODNet [102]，DER [46] 等，在第4.2节中讨论。此外，功能空间上的顺序贝叶斯推理可以视为功能正则化的一种形式，通常需要存储一些旧的训练样本（文献中称为“核心集”），如 FRCL [417], FROMP [326] 和 S-FSVI [378]。

对于条件生成，先前学习条件的生成数据及其输出值在旧模型和新模型之间保持一致性进行正则化，如 MeRGANs [464]，DRI [452] 和 LifelongGAN [500]。
以下是调整了标题级数的内容，使其与前面的内容对应：

### 基于重放的方法

我们将近似和恢复旧数据分布的方法归入这一类（见图5）。根据重放内容的不同，这些方法可以进一步分为三个子方向，每个子方向都有其目标和挑战。

#### 经验重放

经验重放通常在一个小内存缓冲区中存储一些旧的训练样本。由于存储空间极其有限，关键挑战在于如何构建和利用内存缓冲区。

**构建**方面，保存的旧训练样本应该经过精心选择、压缩、增强和更新，以便自适应地恢复过去的信息。早期工作采用固定原则进行样本选择。例如，Reservoir Sampling [67], [369], [430] 随机保留从每个训练批次中获得的固定数量的旧训练样本。Ring Buffer [281]进一步确保每个类别的旧训练样本数量相等。Mean-of-Feature [365]选择最接近每个类别特征均值的等数量旧训练样本。还有许多其他固定原则，例如 k-means [67]，平面距离 [369] 和熵 [369]，但表现一般[67], [369]。

更高级的策略通常是基于梯度或可优化的，通过最大化参数梯度方面的样本多样性（GSS [16]），带有基数约束的对应任务的性能（CCBO [40]），小批量梯度相似性和跨批次梯度多样性（OCS [491]），优化潜在决策边界的能力（ASER [391]），对扰动的鲁棒性多样性（RM [27]），当前参数下旧训练样本梯度的相似性（GCR [418]）等。为了提高存储效率，AQM [48]基于VQ-VAE框架[424]执行在线连续压缩并保存压缩数据进行重放。MRDC [444]将数据压缩的经验重放公式化为行列式点过程（DPPs）[231]，并推导出在线确定适当压缩率的高效方法。RM [27]采用传统和基于标签混合的数据增强策略，以提高旧训练样本的多样性。RAR [234]在遗忘边界附近合成对抗样本，并进行MixUp [504]进行数据增强。低存储成本的辅助信息，如类别统计（IL2M [31]，SNCL [139]）和注意力图（RRR [110]，EPR [380]），可以进一步结合以保持旧知识。此外，旧训练样本可以不断修改以适应增量变化，例如通过使它们更具代表性（Mnemonics [276]）或更具挑战性（GMED [192]）。

**利用**方面，经验重放需要充分利用内存缓冲区以恢复过去的信息。有许多不同的实现，紧密相关于其他连续学习策略。首先，可以约束优化中旧训练样本的效果以避免灾难性遗忘并促进知识迁移。例如，GEM [281]为每个任务基于旧训练样本构建个体约束，以确保它们的损失不增加。A-GEM [66]用所有任务的全局损失代替个体约束，以提高训练效率。LOGD [411]将每个任务的梯度分解为任务共享和任务特定的组件，以利用任务间信息。为了在干扰-迁移（即遗忘-泛化）[369]（即遗忘-泛化[358]）中取得良好的权衡，MER [369]采用元学习进行梯度对齐。BCL [358]明确追求旧训练样本和新训练样本的成本的鞍点。MetaSP [407]利用样本影响的帕累托最优来控制模型和存储更新。为了选择性利用内存缓冲区，MIR [13]优先考虑更多遗忘的旧训练样本，而HAL [64]将它们用作“锚点”并稳定其预测。

另一方面，经验重放可以自然地与知识蒸馏（KD）结合，额外结合旧模型的过去信息。iCaRL [365]和EEIL [53]是两个早期的工作，在旧训练样本和新训练样本上执行KD。一些后续改进专注于经验重放中的不同问题。为了缓解有限旧训练样本的数据不平衡，LUCIR [165]鼓励旧模型和新模型的特征方向相似，同时执行最后一层的余弦归一化和当前任务的难负样本挖掘。BiC [468]和WA [512]将此问题归因于最后一个全连接层的偏差，并通过学习带有平衡验证集的偏差校正层[468]或归一化输出权重[512]来解决。SS-IL [10]在最后一层采用分离的softmax并执行逐任务KD以减轻偏差。DRI [452]训练生成模型以补充旧训练样本的额外生成数据。为了减轻剧烈的表示变化，PODNet [102]采用空间蒸馏损失以保持整个模型的表示。Co2L [57]引入自监督蒸馏损失，以获得对灾难性遗忘具有鲁棒性的表示。GeoDL [396]沿着连接旧特征空间和新特征空间的低维投影路径执行KD，以在它们之间实现平滑过渡。ELI [193]学习旧模型和新模型的能量流形，以重新调整表示变化以优化增量任务。为了充分利用过去的信息，AU [236]在蒸馏损失中结合了不确定性和自注意力，而CSC [21]则额外利用特征空间的结构。DDE [172]从新训练样本的特征中提炼碰撞效应，这在因果上等效于重放更多旧训练样本。TAMiL [36]在特征空间中添加任务特定的注意力并执行一致性正则化，以更好地保持任务相关信息。为了进一步增强学习可塑性，D+R [164]从一个额外模型中执行KD，该模型专用于每个新任务。FOSTER [435]动态扩展新模块以适应旧模型的残差，然后将它们蒸馏到单一模型中。此外，权重正则化方法可以与经验重放结合，以实现更好的性能和通用性[63]，[441]。

值得注意的是，经验重放的优点和潜在局限性仍然很大程度上是开放的。除了直观的旧任务低损失区域的好处外[428]，理论分析表明其对解决最优连续学习的NP难问题的贡献[225]。然而，它有可能过拟合于保存在内存缓冲区中的少量旧训练样本，这可能影响可推广性[428]。为了缓解过拟合，LiDER [39]从对抗鲁棒性中获得灵感，强制模型对输入的Lipschitz连续性。MOCA [493]扩大表示的变化，以防止旧表示在其空间中收缩。有趣的是，几个简单的经验重放基线可以实现相当的性能。DER/DER++ [46]和X-DER [41]保存旧训练样本及其logits，并在优化轨迹中执行logit匹配。GDumb [348]贪婪地在内存缓冲区中收集传入的训练样本，然后从头开始使用它们训练模型以进行测试。这些工作可以作为该子方向中更高级策略的评估标准。

#### 生成重放

生成重放或伪复习通常需要训练一个额外的生成模型以重放生成的数据。这与生成模型本身的连续学习密切相关，因为它们也需要增量更新。DGR [392]提供了一个初始框架，其中每个生成任务的学习伴随着从旧生成模型中采样生成数据进行重放，以继承先前学习的知识。MeRGAN [464]进一步采用重放对齐，通过相同随机噪声采样的生成数据在旧生成模型和新生成模型之间强制一致性，类似于功能正则化的作用。此外，其他连续学习策略可以结合到生成重放中。权重正则化[318], [383], [438], [440]和经验重放[158], [440]已被证明在减轻生成模型的灾难性遗忘方面是有效的。DGMa/DGMw [322]采用二进制掩码分配任务特定参数以克服任务间干扰，并采用可扩展网络以确保可扩展性。如果预训练可用，它可以为连续学习提供相对稳定和强大的参考模型。例如，FearNet [211]和ILCAN [472]额外保存从预训练特征提取器获取的旧特征的统计信息，而GAN-Memory

 [81]则不断调整带有任务特定参数的预训练生成模型。

用于伪复习的生成模型可以是多种类型的，例如生成对抗网络（GANs）和（变分）自动编码器（VAE）。大多数最先进的方法都集中在GANs上，以享受其在细粒度生成方面的优势，但通常在连续学习中遭受标签不一致的困扰[25], [322]。相比之下，基于自动编码器的策略，例如FearNet [211]，SRM [370]，CLEER [375]，EEC [25]，GMR [342]和Flashcards [140]，可以显式控制生成数据的标签，尽管粒度相对模糊。L-VAEGAN [483]则采用混合模型来同时实现高质量生成和精确推理。然而，由于生成模型的连续学习极其困难且需要大量资源开销，生成重放通常限于相对简单的数据集[422], [438]。

#### 特征重放

一种替代方法是将生成重放的目标从数据级别转换到特征级别，这可以大大减少条件生成的复杂性，并更充分地利用语义信息。例如，GFR [273]训练条件GANs在特征提取器之后重放生成的特征。BI-R [422]在标准VAE中结合上下文调制的反馈连接，以重放内部表示。实际上，维护特征级别而非数据级别的分布在效率和隐私方面具有许多优势。我们将这一子方向称为特征重放。然而，一个核心挑战是特征提取器的顺序更新引起的表示变化，这反映了特征级别的灾难性遗忘。为了解决这个问题，GFR [273]，FA [181] 和 DSR [525] 在旧模型和新模型之间执行特征蒸馏。IL2M [31]和SNCL [139] 基于经验重放恢复特征表示的统计信息（例如均值和协方差）。RER [419]显式估计表示变化以更新保存的旧特征。REMIND [156] 和 ACAE-REMIND [437]则固定特征提取器的早期层并重构中间表示以更新后续层。FeTrIL [341]采用从初始任务中学习的固定特征提取器，并在之后重放生成的特征。对于从头开始的连续学习，表示的变化通常是剧烈的，而稳定特征提取器可能会干扰适应新表示。相比之下，使用强预训练可以提供对下游任务具有鲁棒性且在连续学习中相对稳定的表示。一项经验研究[321]系统地研究了大规模预训练的连续学习中特征重放。关于这一主题的更深入讨论将在第4.4节中提出。

###  基于优化的方法

连续学习不仅可以通过向损失函数添加额外项（例如正则化和重放）来实现，还可以通过显式设计和操作优化程序来实现。一个典型的想法是执行梯度投影。一些基于重放的方法，如 GEM [281]、A-GEM [66]、LOGD [411] 和 MER [369]，通过使用一些旧训练样本来保持先前输入空间和梯度空间，限制参数更新以与经验重放方向对齐。与保存旧训练样本不同，OWM [496] 和 AOP [146] 修改参数更新，使其与先前输入空间的正交方向对齐。OGD [115] 保持旧梯度方向并使当前梯度方向与其正交。Orthog-Subspace [65] 通过正交低秩向量子空间执行连续学习，使不同任务的梯度彼此正交。GPM [380] 维护对旧任务重要的梯度子空间（即核心梯度空间的基）以进行参数更新的正交投影。CGP [68] 从各个类中计算这种梯度子空间，以进一步减轻类间干扰。FS-DGPM [90] 动态释放 GPM [380] 的不重要基，以提高学习可塑性并鼓励收敛到平坦的损失景观。CUBER [261] 选择性地投影梯度以更新与当前任务正相关的旧任务知识。TRGP [260] 通过将梯度投影到先前输入子空间的范数上定义“信任区域”，从而选择性地重用旧任务的冻结权重。Adam-NSCL [448] 则将候选参数更新投影到当前通过旧任务无中心特征协方差近似的零空间，而 AdNS [227] 则考虑了先前和当前零空间的共享部分。NCL [202] 统一了贝叶斯权重正则化和梯度投影，鼓励参数更新在旧任务的零空间中，同时收敛到贝叶斯近似后验的最大值。在贝叶斯权重正则化的二次惩罚上限下，RGO [266] 通过递归优化程序修改梯度方向以获得最优解。因此，正则化和重放最终是通过修正当前梯度方向实现的，而梯度投影则是显式地在参数更新中进行类似的修改。

图6. 基于优化的方法。该方向的特点是显式设计和操作优化程序，例如参考旧任务的梯度空间或输入空间进行梯度投影（改编自[115]），在内循环中对顺序到达的任务进行元学习，以及利用模式连通性和损失景观中的平坦极小值（改编自[258], [312]）。θ∗A、θ∗B 和 θ∗A,B 分别是任务A、任务B 和两者的理想解决方案。

另一个有吸引力的想法是元学习或学习如何学习连续学习，它试图为各种场景获得数据驱动的归纳偏置，而不是手动设计[150]。OML [186] 提供了一种元训练策略，对顺序到达的输入进行在线更新，并最小化它们的干扰，从而自然获得适合连续学习的稀疏表示。ANML [30] 通过元学习一个上下文依赖的门控函数来选择性激活与增量任务相关的神经元，扩展了这一思想。AIM [238] 学习专家混合体，以 OML [186] 或 ANML [30] 的表示进行预测，进一步在架构层面稀疏化表示。同时，元学习可以与经验重放结合，以更好地利用旧的和新的训练样本。例如，MER [369] 使它们的梯度方向对齐，而 iTAML [357] 应用元更新规则以保持它们的平衡。在经验重放的帮助下，La-MAML [148] 以自适应调整的学习率在线优化 OML [186] 目标。OSAKA [49] 提出了一种混合目标，结合知识积累和快速适应，通过元训练获得良好的初始化，然后将增量任务的知识纳入初始化。元学习还可以用于优化专门的架构。MERLIN [223] 在每个任务的表示下巩固模型参数的元分布，允许采样任务特定的模型并集成它们进行推理。类似地，PR [161] 采用贝叶斯策略学习具有共享元模型的任务特定后验。MARK [176] 维护一组共享权重，通过元学习进行增量更新，并选择性掩码以解决特定任务。ARI [446] 将对抗攻击与经验重放结合，获得任务特定模型，然后通过元训练将它们融合在一起。

此外，还有一些其他工作从损失景观的角度优化过程。例如，不是专门设计一个算法，Stable-SGD [313] 通过调整训练方案中的因素（如 dropout、学习率衰减和批量大小），使 SGD 找到一个平坦的局部极小值。MC-SGD [312] 实验证明，多任务学习（即联合训练所有增量任务）和连续学习获得的局部极小值可以通过低误差的线性路径连接起来，并应用经验重放沿着该路径找到更好的解决方案。Linear Connector [258] 采用 Adam-NSCL [448] 和特征蒸馏，获得通过低误差线性路径连接的旧任务和新任务的各自解决方案，然后进行线性平均。此外，无监督/自监督学习（比传统的监督学习）[127], [168], [286] 和大规模预训练（比随机初始化）[84], [168], [300], [360], [467] 被证明在减轻灾难性遗忘方面更少。经验上，两者都归因于获得更鲁棒（例如正交、稀疏和均匀分散）的表示[168], [286], [360], [389]，并收敛到更宽的损失盆地[152], [168], [286], [300], [317]，这表明表示、参数和任务特定误差的敏感性之间可能存在潜在联系。许多努力寻求在连续学习中利用这些优势，正如我们接下来讨论的那样。
以下是调整了标题级数的内容，使其与前面的内容对应：

### 基于表示的方法

我们将创建和利用表示优势的连续学习方法归为这一类。除了早期通过元训练获取稀疏表示的工作[186]，最近的研究尝试结合自监督学习（SSL）[127], [286], [343] 和大规模预训练 [300], [389], [467] 的优势，以改善初始化和连续学习中的表示。需要注意的是，这两种策略密切相关，因为预训练数据通常数量巨大且没有显式标签，而SSL本身的性能主要通过在下游任务（或任务序列）上微调来评估。以下我们将讨论代表性的子方向。

#### 自监督学习

自监督学习（基本上使用对比损失）用于连续学习。观察到自监督表示对灾难性遗忘更具鲁棒性，LUMP [286] 通过在旧任务和新任务实例之间插值获得进一步改进。MinRed [349] 通过去相关存储的旧训练样本来进一步促进经验重放的多样性。CaSSLe [118] 通过将表示的当前状态映射到其之前状态，将自监督损失转换为蒸馏策略。Co2L [57] 采用监督对比损失来学习每个任务，并采用自监督损失在旧模型和新模型之间蒸馏知识。DualNet [343] 使用监督损失训练一个快速学习者，并使用自监督损失训练一个慢速学习者，后者帮助前者获取可推广的表示。

#### 预训练

预训练用于下游连续学习。若干实证研究表明，预训练不仅带来了强大的知识转移，还提高了对灾难性遗忘的鲁棒性，从而明显有利于下游连续学习[127], [300], [321], [360]。特别是，当使用更大规模的数据[321], [360]，更大规模的模型[360] 和对比损失[84], [127] 进行预训练时，下游连续学习的收益更加明显。然而，一个关键挑战是需要自适应地利用预训练知识以适应当前任务，同时保持对未来任务的可推广性。对于这个问题的各种策略取决于预训练的骨干网络是否固定。

图7. 基于表示的方法。该方向的特点是创建和利用表示的优势进行连续学习，例如通过使用自监督学习和预训练。特别是，上游预训练和下游微调都需要进行连续学习，而预训练表示在学习特定下游任务时可选择性固定。

#### 适应固定的骨干网络

Side-Tuning [506] 和 DLCFT [394] 训练一个轻量级网络与骨干网络并行，并线性融合它们的输出。TwF [42] 也训练一个兄弟网络，但在层级方式上从骨干网络中蒸馏知识。GAN-Memory [81] 利用 FiLM [339] 和 AdaFM [515] 为预训练生成模型的每一层学习任务特定参数，而 ADA [113] 采用 Adapters [166] 与知识蒸馏来调整预训练转换器。最近的基于提示的方法通过少量提示参数来指导预训练转换器的表示。此类方法通常涉及构建任务自适应提示和推理合适提示以进行测试，通过探索提示架构来适应任务共享和任务特定知识。代表性策略包括从提示池中选择最相关的提示（L2P [458]），使用注意力因子对提示池进行加权求和（CODAPrompt [400]），使用明确的任务共享和任务特定提示（DualPrompt [457]）或仅任务特定提示（S-Prompts [450], HiDe-Prompt [439]），任务特定提示的逐步扩展（Progressive Prompts [364]）等。此外，通过保存原型，在骨干网络后附加最近类均值（NCM）分类器已被证明是一个强大的基线[185], [332]，可以通过传递学习技术（如 FiLM 适配器[327]）进一步增强。

#### 优化可更新的骨干网络

F2M [387] 在预训练阶段搜索平坦的局部极小值，然后在平坦区域内学习增量任务。CwD [389] 规范初始阶段的表示均匀散布，可以经验性地模拟联合训练的表示。SAM [122], [300] 鼓励通过优化平坦度指标在下游连续学习中找到宽广的盆地。SLCA [503] 观察到缓慢微调预训练转换器的骨干可以在连续学习中取得出色表现，并进一步保留原型统计以纠正输出层。

#### 连续预训练（CPT）或连续元训练

由于预训练所需的数据量通常以增量方式收集，执行上游连续学习以提高下游性能尤为重要。例如，一项实证研究发现，自监督预训练比监督协议更适合视觉-语言模型的连续学习[82]，这与仅视觉任务的结果一致[168]。由于文本通常比图像更高效，IncCLIP [478] 根据图像重放生成的难负样本文本，并执行多模态知识蒸馏以更新CLIP [355]。对于语言模型的CPT，ECONET [151] 设计了一个带有生成重放的自监督框架。同时，连续元训练需要解决类似的问题，即基础类的预训练知识以增量方式丰富和适应。IDA [268] 施加新旧模型相对于旧中心的判别对齐，并使嵌入自由适应新任务。ORDER [456] 采用无标签的OOD数据与经验重放和特征重放相结合，以应对任务间的巨大差异。
以下是调整了标题级数的内容，使其与前面的内容对应：

### 基于架构的方法

上述策略主要集中于使用共享参数集（即单个模型和一个参数空间）来学习所有增量任务，这是任务间干扰的主要原因。相反，构建任务特定的参数可以明确解决这个问题。之前的工作通常将这一类别分为参数隔离和动态架构，取决于网络架构是否固定。在这里，我们集中讨论实现任务特定参数的方法，将上述概念扩展到参数分配、模型分解和模块化网络（见图8）。

#### 参数分配

参数分配特点是通过网络为每个任务分配一个独立的参数子空间，架构可以是固定的也可以是动态的。在固定的网络架构中，Piggyback [290], HAT [384], SupSup [463], MEAT [477], WSN [199] 和 H2 [190] 通过优化二进制掩码来选择每个任务的专用神经元或参数，同时冻结旧任务的掩码区域以防止灾难性遗忘。PackNet [291], UCL [9], CLNP [138], AGS-CL [196] 和 NISPA [149] 明确识别当前任务的重要神经元或参数，然后释放不重要部分给后续任务，这可以通过迭代修剪[291]、激活值[138], [149], [196]、不确定性估计[9]等实现。由于网络容量有限，随着引入更多增量任务，“空闲”参数往往会饱和。因此，这些方法通常需要在参数使用上施加稀疏性约束，并选择性地重用冻结的旧参数，这可能会影响每个任务的学习。为缓解这一困境，如果网络容量不足以良好学习新任务，网络架构可以动态扩展，如DEN [492], CPG [175] 和 DGMa/DGMw [322]。动态架构还可以通过强化学习（RCL [475], BNS [352]）、架构搜索（LtG [254], BNS [352]）、变分贝叶斯（BSA [232]）等优化，以提高参数效率和知识转移。由于网络扩展速度应远慢于任务增加速度以确保可扩展性，稀疏性和可重用性约束仍然重要。

图8. 基于架构的方法。该方向的特点是通过适当设计的架构构建任务特定/自适应参数，例如为每个任务分配专用参数（参数分配）、构建任务自适应子模块或子网络（模块化网络）、将模型分解为任务共享和任务特定组件（模型分解）。这里展示了两种类型的模型分解，对应参数（低秩分解，改编自[178]）和表示（中间特征的掩码）。

#### 模型分解

模型分解明确将模型分为任务共享和任务特定组件，任务特定组件通常是可扩展的。对于常规网络，任务特定组件可以是并行分支（ACL [109], ReduNet [470], EPIE-Net [106]）、自适应层（GVCL [280], DyTox [103]）、中间特征的掩码或掩码生成器（CCLL [398], CCG [1], MARK [176]）。注意，模型分解的特征掩码不在参数空间中操作，并且不是每个任务的二进制掩码，因此与参数分配的二进制掩码本质上不同。此外，网络参数本身可以分解为任务共享和任务特定元素，例如通过加法分解（APD [490]）、奇异值分解（RCM [198]）、滤波器原子分解（FAS [304]）和低秩分解（IBP-WF [299], IRU [178]）。由于任务特定组件的数量通常随增量任务线性增长，它们的资源效率决定了这一子方向的可扩展性。

#### 模块化网络

模块化网络利用并行子网络或子模块以区分的方式学习增量任务，而不预定义任务共享或任务特定组件。作为早期工作，Progressive Networks [379] 为每个任务引入一个相同的子网络，并通过适配器连接允许知识转移。Expert Gate [14] 和后续工作[80] 使用专家混合体[184]学习增量任务，每引入一个任务就扩展一个专家。PathNet [117] 和 RPSNet [356] 预先分配多个并行网络以构建一些候选路径，即网络模块的分层组合，并为每个任务选择最佳路径。MNTDP [427] 和 LMC [323] 尝试从先前的子模块和潜在的新子模块中找到最佳布局。与参数分配类似，这些努力有意构建任务特定模型，同时子网络或子模块的组合允许明确的知识重用。此外，可以鼓励子网络并行学习增量任务。Model Zoo [361] 扩展子网络以通过经验重放学习每个新任务，并集成所有子网络进行预测。CoSCL [443] 和 CAF [442] 集成多个连续学习模型并调节它们之间的预测相似性，证明在解决任务分布差异和改善损失景观平坦性方面有效。

#### 任务特定/自适应参数

从广义上讲，从任务条件参数分布中采样参数（MERLIN [223], PR [161], PGMA [170], HNET [431]），以及通过权重正则化稳定重要参数，可以视为导出任务特定/自适应参数的一种形式。与其他方向相比，大多数基于架构的方法相当于在网络参数中使增量任务去相关，这几乎可以避免灾难性遗忘，但影响可扩展性和任务间可推广性。特别是，任务标识通常需要确定使用哪组参数，从而大大限制其应用。为克服这一限制，可以从每个任务特定模型的响应（例如预测不确定性）中推断任务标识[14], [161], [218]。任务标识预测的功能还可以通过增量任务显式学习，使用其他连续学习策略来减轻灾难性遗忘[1], [80], [109], [161], [190], [223], [299]。

此外，基本架构的设计和选择对连续学习性能有很大影响。例如，由于梯度方向中更多的正交性和稀疏性以及较慢的训练机制，较宽的网络往往对灾难性遗忘更具鲁棒性[309], [310]。批量归一化（BN）层[180]往往引入对当前任务的偏差矩，这导致先前任务的次优性能[59], [284], [345]。在稳定网络中的Dropout[403]表现得像一个门控机制，创建任务特定路径，从而减轻灾难性遗忘[311]。