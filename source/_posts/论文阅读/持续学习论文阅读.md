--- 
 title: MKGFormer 
 date: 2024-07-05 14:53:42 
 updated: 2024-07-13
 mathjax: true
tags:
  - 持续学习
  - 论文阅读
---

## 综述
### Continual Learning Scenarios

#### Instance-Incremental Learning (IIL)
- **定义**: 所有训练样本属于相同任务，以批次到达。
- **训练**: {Dt,b, t}b∈Bt
- **测试**: {p(Xt)}t=j；不需要任务标识

#### Domain-Incremental Learning (DIL)
- **定义**: 任务具有相同的数据标签空间但不同的输入分布。任务标识不是必须的。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj) 且 Yi = Yj 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；不需要任务标识

#### Task-Incremental Learning (TIL)
- **定义**: 任务具有不相交的数据标签空间。在训练和测试中提供任务标识。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；提供任务标识

#### Class-Incremental Learning (CIL)
- **定义**: 任务具有不相交的数据标签空间。仅在训练中提供任务标识。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；不提供任务标识

#### Task-Free Continual Learning (TFCL)
- **定义**: 任务具有不相交的数据标签空间。在训练和测试中均不提供任务标识。
- **训练**: \{\{Dt,b\}b∈Bt\}t∈T；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；任务标识可选

#### Online Continual Learning (OCL)
- **定义**: 任务具有不相交的数据标签空间。每个任务的训练样本以一次性数据流形式到达。
- **训练**: \{\{Dt,b\}b∈Bt\}t∈T, |b| = 1；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；任务标识可选

#### Blurred Boundary Continual Learning (BBCL)
- **定义**: 任务边界模糊，具有不同但重叠的数据标签空间。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj), Yi ̸= Yj 且 Yi ∩ Yj ̸= ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；不提供任务标识

#### Continual Pre-training (CPT)
- **定义**: 预训练数据按顺序到达。目标是改进知识传递到下游任务。
- **训练**: {Dpt t , t}t∈T pt , 随后是下游任务 j
- **测试**: {p(Xt)}t=j；不需要任务标识

#### 其他学习场景
除非特别说明，每个任务通常假定有足够数量的标记训练样本，即监督连续学习。根据每个 Dt 中提供的 Xt 和 Yt，连续学习进一步扩展到零样本学习、少样本学习、半监督学习、开放世界学习（识别未知类别并整合其标签）和无监督/自监督学习场景。此外，还考虑并纳入了其他实际挑战，例如多标签、噪声标签、层次粒度和子群体、任务相似性的混合、长尾分布、域对齐、域迁移、随时推理、新类别发现、多模态等。一些最新的工作集中在这些场景的各种组合上，以更好地模拟现实世界的复杂性。

### 连续学习的性能评估

通常，连续学习的性能可以从三个方面进行评估：已经学习任务的总体性能、旧任务的记忆稳定性和新任务的学习可塑性。以下是常见的评估指标，使用分类作为示例。

#### 总体性能
总体性能通常通过平均准确率 (AA) 和平均增量准确率 (AIA) 来评估。

设 \( a_{k,j} \in [0, 1] \) 表示在增量学习第 \( k \) 个任务后，在第 \( j \) 个任务的测试集上评估的分类准确率 (\( j \leq k \))。计算 \( a_{k,j} \) 的输出空间包括 \( Y_j \) 或 \(\bigcup_{i=1}^{k} Y_i\)，分别对应多头评估 (如 TIL) 或单头评估 (如 CIL)。

第 \( k \) 个任务的两个指标定义如下：
$$
AA_k = \frac{1}{k} \sum_{j=1}^{k} a_{k,j}
$$
$$
AIA_k = \frac{1}{k} \sum_{i=1}^{k} AA_i
$$
其中，AA 代表当前时刻的总体性能，AIA 进一步反映了历史性能。

#### 记忆稳定性
记忆稳定性可以通过遗忘度量 (FM) 和向后传递 (BWT) 来评估。

对于前者，任务的遗忘通过其过去的最大性能与当前性能的差值计算：
$$
f_{j,k} = \max_{i \in \{1,...,k-1\}} (a_{i,j} - a_{k,j}), \forall j < k
$$
第 \( k \) 个任务的 FM 是所有旧任务的平均遗忘：
$$
FM_k = \frac{1}{k-1} \sum_{j=1}^{k-1} f_{j,k}
$$
对于后者，BWT 评估学习第 \( k \) 个任务对所有旧任务的平均影响：
$$
BWT_k = \frac{1}{k-1} \sum_{j=1}^{k-1} (a_{k,j} - a_{j,j})
$$
其中，遗忘通常反映为负的 BWT。

#### 学习可塑性
学习可塑性可以通过不易学习度量 (IM) 和向前传递 (FWT) 来评估。

IM 定义为模型学习新任务的能力不足，通过任务的联合训练性能与持续学习性能之间的差异计算：
$$
IM_k = a^*_k - a_{k,k}
$$
其中，\( a^*_k \) 是随机初始化的参考模型在联合训练 \(\bigcup_{j=1}^{k} D_j\) 时第 \( k \) 个任务的分类准确率。相比之下，FWT 评估所有旧任务对当前第 \( k \) 个任务的平均影响：
$$
FWT_k = \frac{1}{k-1} \sum_{j=2}^{k} (a_{j,j} - \tilde{a}_j)
$$
其中，\( \tilde{a}_j \) 是随机初始化的参考模型在第 \( j \) 个任务的 \( D_j \) 上训练的分类准确率。

#### 其他评估指标
请注意，\( a_{k,j} \) 可以根据任务类型适应其他形式，例如对象检测的平均精度 (AP)、语义分割的交并比 (IoU)、图像生成的 Frechet Inception Distance (FID)、强化学习的标准化奖励等，并应相应调整上述评估指标。此外，还有许多其他有用的指标，例如表示遗忘的线性探针、Hessian 矩阵的最大特征值用于损失景观的平坦性、任何时候推理的准确率曲线下面积、存储和计算的开销用于资源效率等。我们建议读者参考其原始论文。
### 理论基础

在本节中，我们总结了关于连续学习的理论工作，涉及稳定性-可塑性权衡和可推广性分析，并将其与各种连续学习方法的动机联系起来。

#### 稳定性-可塑性权衡

根据第2.1节的基本公式，我们考虑一个通用的连续学习设置，其中具有参数 \(\theta \in \mathbb{R}^{|\theta|}\) 的神经网络需要学习 \(k\) 个增量任务。假设每个任务的训练集和测试集遵循相同的分布 \(D_t, t = 1, ..., k\)，其中训练集 \(D_t = \{X_t, Y_t\} = \{(x_{t,n}, y_{t,n})\}_{n=1}^{N_t}\) 包含 \(N_t\) 对数据标签对。目标是学习一个概率模型 \(p(D_{1:k}|\theta) = \prod_{t=1}^{k} p(D_t|\theta)\)（在条件独立假设下），该模型能够在所有任务上表现良好，记为 \(D_{1:k} := \{D_1, ..., D_k\)）。

判别模型的任务相关性能可以表示为 \(\log p(D_t|\theta) = \sum_{n=1}^{N_t} \log p_\theta(y_{t,n}|x_{t,n})\)。连续学习的核心挑战通常来自学习的顺序性质：在从 \(D_k\) 学习第 \(k\) 个任务时，旧的训练集 \(\{D_1, ..., D_{k-1}\}\) 是不可访问的。因此，关键但困难的是在捕获旧任务和新任务的分布时保持平衡，即确保适当的稳定性-可塑性权衡，其中过度的学习可塑性或记忆稳定性可能会大大妨碍对方（见图2，a，b）。

一个直接的想法是通过存储一些旧的训练样本或训练生成模型来近似和恢复旧的数据分布，这在第4.2节中称为基于重放的方法。根据监督学习的学习理论[154]，通过重放更多近似其分布的旧训练样本来提高旧任务的性能，但这会导致潜在的隐私问题和资源开销的线性增长。生成模型的使用也受到额外资源开销、其自身的灾难性遗忘和表达能力的限制。

另一种选择是通过在贝叶斯框架中公式化连续学习来传播旧数据分布。在网络参数的先验 \(p(\theta)\) 基础上，观察第 \(k\) 个任务后的后验可以通过贝叶斯规则计算：
$$
p(\theta|D_{1:k}) \propto p(\theta) \prod_{t=1}^{k} p(D_t|\theta) \propto p(\theta|D_{1:k-1})p(D_k|\theta),
$$
其中，第 \(k-1\) 个任务的后验 \(p(\theta|D_{1:k-1})\) 成为第 \(k\) 个任务的先验，因此仅使用当前训练集 \(D_k\) 就可以计算新的后验 \(p(\theta|D_{1:k})\)。

然而，由于后验通常是不可解的（除非在非常特殊的情况下），常见的选择是用 \(q_{k-1}(\theta) \approx p(\theta|D_{1:k-1})\) 来近似，同样对于 \(q_k(\theta) \approx p(\theta|D_{1:k})\)。接下来，我们将介绍两种广泛使用的近似策略：

第一种是在线拉普拉斯近似，它使用局部梯度信息将 \(p(\theta|D_{1:k-1})\) 近似为多元高斯[177]，[202]，[222]，[371]，[441]。具体来说，我们可以用 \(\phi_{k-1}\) 参数化 \(q_{k-1}(\theta)\) 并通过在 \(p(\theta|D_{1:k-1})\) 的模态 \(\mu_{k-1} \in \mathbb{R}^{|\theta|}\) 周围执行二阶泰勒展开构建一个近似高斯后验 \(q_{k-1}(\theta) := q(\theta; \phi_{k-1}) = \mathcal{N}(\theta; \mu_{k-1}, \Lambda_{k-1}^{-1})\)，其中 \(\Lambda_{k-1}\) 表示精度矩阵，\(\phi_{k-1} = \{\mu_{k-1}, \Lambda_{k-1}\}\)，同样对于 \(q(\theta; \phi_k), \mu_k 和 \Lambda_k\)。

根据公式(8)，学习当前第 \(k\) 个任务的后验模态可以计算为：
$$
\mu_k = \arg \max_\theta \log p(\theta|D_{1:k}) \approx \arg \max_\theta \log p(D_k|\theta) + \log q(\theta; \phi_{k-1}) = \arg \max_\theta \log p(D_k|\theta) - \frac{1}{2} (\theta - \mu_{k-1})^\top \Lambda_{k-1}(\theta - \mu_{k-1}),
$$
该更新递归自 \(\mu_{k-1} 和 \Lambda_{k-1}\)。同时，\(\Lambda_k\) 从 \(\Lambda_{k-1}\) 递归更新：
$$
\Lambda_k = -\nabla^2_\theta \log p(\theta|D_{1:k})|_{\theta=\mu_k} \approx -\nabla^2_\theta \log p(D_k|\theta)|_{\theta=\mu_k} + \Lambda_{k-1},
$$
其中右侧的第一项是负对数似然的海森矩阵，记为 \(H(D_k, \mu_k)\)。实际上，由于 \(\mathbb{R}^{|\theta|}\) 的高维性，\(H(D_k, \mu_k)\) 通常计算效率低，并且没有保证近似的 \(\Lambda_k\) 对于高斯假设是半正定的。为了解决这些问题，海森矩阵通常用费舍尔信息矩阵（FIM）来近似：
$$
F_k = \mathbb{E}[\nabla_\theta \log p(D_k|\theta) \nabla_\theta \log p(D_k|\theta)^\top]|_{\theta=\mu_k} \approx H(D_k, \mu_k)。
$$
为了便于计算，FIM 可以进一步简化为对角近似[177]，[222] 或克罗内克分解近似[293]，[371]。然后，公式(9) 通过保存旧模型 \(\mu_{k-1}\) 的冻结副本来实现，以规避参数变化，称为第4.1节中的基于正则化的方法。这里我们以 EWC [222] 为例，给出其损失函数：
$$
L_{EWC}(\theta) = \ell_k(\theta) + \frac{\lambda}{2} (\theta - \mu_{k-1})^\top \hat{F}_{1:k-1}(\theta - \mu_{k-1}),
$$
其中 \(\ell_k\) 表示特定任务的损失，FIM \(\hat{F}_{1:k-1} = \sum_{t=1}^{k-1} \text{diag}(F_t)\) 使用每个 \(F_t\) 的对角近似 \(\text{diag}(\cdot)\)，\(\lambda\) 是控制正则化强度的超参数。

第二种是在线变分推理（VI）[7]，[91]，[206]，[235]，[248]，[280]，[318]，[378]，[410]。有很多不同的方法来实现这一点，其中一个代表性的方法是最小化当前第 \(k\) 个任务在满足 \(p(\theta|D_{1:k}) \in Q\) 的家族 Q 上的以下 KL 散度：
$$
q_k(\theta) = \arg \min_{q \in Q} KL(q(\theta) \parallel \frac{1}{Z_k} q_{k-1}(\theta)p(D_k|\theta)),
$$
其中 \(Z_k\) 是 \(q_{k-1}(\theta)p(D_k|\theta)\) 的归一化常数。实际上，上述最小化可以通过使用附加的蒙特卡罗近似来实现，并指定 \(q_k(\theta) := q(\theta; \phi_k) = \mathcal{N}(\theta; \mu_k, \Lambda_k^{-1})\) 为多元高斯。这里我们以 VCL [318] 为例，它最小化以下目标（即最大化其负值）：
$$
L_{VCL}(q_k(\theta)) = \mathbb{E}_{q_k(\theta)}(\ell_k(\theta)) + KL(q_k(\theta) \parallel q_{k-1}(\theta))，
$$
其中 KL 散度可以以闭式形式计算，并作为隐含的正则化项。特别地，尽管公式(12) 和公式(14) 的损失函数形式相似，前者是基于一组确定参数 \(\theta\) 的局部近似，而后者是通过从变分分布 \(q_k(\theta)\) 采样计算的。这归因于两种近似策略之间的基本差异[318]，[420]，在特定设置中表现略有不同。

除了参数空间，顺序贝叶斯推理的思想也适用于函数空间[326]，[378]，[417]，这往往能够在更新参数时提供更多的灵活性。此外，还有许多 VI 的其他扩展，例如通过变分自回归高斯过程（VAR-GPs）改善后验更新[206]，构建任务特定参数[7]，[232]，[244]，[280]，以及适应非平稳数据流[235]。

本质上，无论是重放还是正则化，连续学习的约束最终都体现在梯度方向上。因此，一些最新的工作直接操纵基于梯度的优化过程，归类为第4.3节中的基于优化的方法。具体来说，当在内存缓冲区中保留任务 \(t\) 的一些旧训练样本 \(M_t\) 时，鼓励新训练样本的梯度方向与 \(M_t\) 的梯度方向保持接近[66]，[281]，[411]。这被公式化为
$$
\langle\nabla_\theta L_k(\theta; D_k), \nabla_\theta L_k(\theta; M_t)\rangle \geq 0 \quad \text{for} \; t \in \{1, ..., k-1\},
$$
以实质上强制旧任务的损失不增加，即 \(L_k(\theta; M_t) \leq L_k(\theta_{k-1}; M_t)\)，其中 \(\theta_{k-1}\) 是学习第 \(k-1\) 个任务结束时的网络参数。

或者，梯度投影也可以在不存储旧训练样本的情况下进行[65]，[115]，[146]，[202]，[227]，[258]，[266]，[333]，[380]，[448]，[496]。这里我们以 NCL [202] 为例，它在在线拉普拉斯近似中使用 \(\mu_{k-1}\) 和 \(\Lambda_{k-1}\) 操作梯度方向。如公式(15)所示，NCL 通过在以 \(\theta\) 为中心的半径 \(r\) 区域内的距离度量 \(d(\theta, \theta + \delta) = \sqrt{\delta^\top \Lambda_{k-1} \delta / 2}\) 进行最小化任务特定损失 \(\ell_k(\theta)\) 来执行连续学习，该距离度量考虑了先验的曲率通过其精度矩阵 \(\Lambda_{k-1}\)：
$$
\delta^* = \arg \min_\delta \ell_k(\theta + \delta) \approx \arg \min_\delta \ell_k(\theta) + \nabla_\theta \ell_k(\theta)^\
$$
### 可推广性分析

当前的理论工作主要在增量任务的训练集上进行，假设它们的测试集遵循相似的分布，并且候选解决方案具有相似的可推广性。然而，由于学习多个任务的目标通常是高度非凸的，在每个训练集上表现相似但在测试集上具有显著不同可推广性的局部最优解很多[313], [443]。对于连续学习，理想的解决方案不仅需要从训练集到测试集的任务内可推广性，还需要适应其分布增量变化的任务间可推广性。

图2显示了连续学习的关键因素分析。a和b中，连续学习需要在学习可塑性和记忆稳定性之间保持适当的平衡，其中任一方面的过剩都会影响总体性能（改编自[443]）。c和d中，当收敛的损失景观更平坦且观察到的数据分布更相似时，适当平衡的解决方案可以更好地推广到任务序列（改编自[443]）。e中，参数空间的结构决定了找到理想解决方案的复杂性和可能性（改编自[225]）。黄色区域表示单个任务共享的可行参数空间，随着更多增量任务的引入，该空间往往变窄且不规则。

这里提供一个任务特定损失 \(\ell_t(\theta; D_t)\) 及其经验最优解 \(\theta^*_t = \arg \min_\theta \ell_t(\theta; D_t)\) 的概念演示。当任务 \(i\) 需要适应另一个任务 \(j\) 时，可以通过在 \(\theta^*_i\) 周围对 \(\ell_i(\theta; D_i)\) 进行二阶泰勒展开来估计其性能的最大牺牲：
$$
\ell_i(\theta^*_j; D_i) \approx \ell_i(\theta^*_i; D_i) + (\theta^*_j - \theta^*_i)^\top \nabla_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i} + \frac{1}{2} (\theta^*_j - \theta^*_i)^\top \nabla^2_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i} (\theta^*_j - \theta^*_i) \approx \ell_i(\theta^*_i; D_i) + \frac{1}{2} \Delta \theta^\top \nabla^2_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i} \Delta \theta,
$$
其中 \(\Delta \theta := \theta^*_j - \theta^*_i\) 并且 \(\nabla_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i} \approx 0\)。那么，任务 \(i\) 的性能退化上限为：
$$
\ell_i(\theta^*_j; D_i) - \ell_i(\theta^*_i; D_i) \leq \frac{1}{2} \lambda_{\max i} \|\Delta \theta\|^2,
$$
其中 \(\lambda_{\max i}\) 是海森矩阵 \(\nabla^2_\theta \ell_i(\theta; D_i) \big|_{\theta=\theta^*_i}\) 的最大特征值。注意，任务 \(i\) 和 \(j\) 的顺序可以是任意的，即公式(20) 说明了前向和后向效果。因此，经验最优解 \(\theta^*_i\) 对参数变化的鲁棒性与 \(\lambda_{\max i}\) 密切相关，这已经成为描述损失景观平坦性的常用指标[163]，[212]，[313]。

直观地，收敛到具有更平坦损失景观的局部极小值对适度的参数变化不太敏感，因此有利于旧任务和新任务（见图2，c）。为了找到这样的平坦极小值，连续学习中有三种广泛使用的策略。

第一种来源于其定义，即平坦性指标。具体来说，可以将 \(\ell_t(\theta; D_t)\) 的最小化替换为鲁棒的任务特定损失 \(\ell_{tb}(\theta; D_t) := \max_{\|\delta\|\leq b} \ell_t(\theta + \delta; D_t)\)，因此获得的解不仅在特定点而且在其邻域内具有“半径” \(b\) 的低误差。然而，由于 \(\theta\) 的高维性，\(\ell_{tb}(\theta; D_t)\) 的计算无法覆盖所有可能的 \(\delta\)，只能覆盖少数方向[387]，类似于公式(19)中计算海森矩阵的复杂性问题。替代方法包括使用海森矩阵的近似[90]，[313] 或仅沿前向和后向参数变化的轨迹计算 \(\delta\)[58]，[174]，[183]，[300]，[312]。

第二种是在模式连通性的限制下，通过构建集成模型来操作损失景观，即沿低误差路径在参数或函数空间中集成多个极小值，因为连接它们可以确保该路径上的平坦性[60]，[131]，[183]，[312]，[443]，[462]。这两种策略与基于优化的方法密切相关。

第三种是获得分布良好的表示，这些表示在函数空间中的分布差异上更具鲁棒性，例如通过使用预训练[168]，[300]，[360]，更宽的网络架构[309]，[359]，[360] 和自监督学习[57]，[168]，[286]，[343]。观察到对大规模预训练和自监督学习的高度关注，我们将这一方向归为第4.4节中的基于表示的方法。

还有许多其他因素对连续学习的性能很重要。如公式(20)所示，性能退化的上限还取决于每个任务的经验最优解 \(\theta^*_t = \arg \min_\theta \ell_t(\theta; D_t)\) 的差异，即任务分布的差异（见图2，d），这一点通过遗忘-泛化权衡的理论分析[358]和泛化误差的PAC-Bayes界[338]，[443]进一步验证。因此，如何利用任务相似性与连续学习的性能直接相关。每个任务的泛化误差可以通过协同任务提高，但在竞争任务中恶化[361]，其中在共享解决方案中同等学习所有任务往往会在性能上妥协每个任务[361]，[443]。

另一方面，当模型参数不被所有任务共享（例如，使用多头输出层）时，任务相似性对连续学习的影响将会复杂。一些使用神经切线核（NTK）的理论研究[33]，[95]，[207]，[243]表明，任务相似性的增加可能导致更多的遗忘。由于输出头是独立的，因此更难区分两个相似的解决方案[207]，[243]。具体来说，在NTK体制下，从第 \(t\) 个任务到第 \(k\) 个任务，旧任务的遗忘有界于：
$$
\|p(D_k |\theta^*_k) - p(D_k|\theta^*_t )\|_F^2 \leq \sigma^2_{t,|rep|} + \sum_{i=t+1}^{k} \left( \Theta_{t \to S(i,|rep|)} \right)^2 \|\text{RES}_i\|_2^2。
$$
\(\Theta_{t \to k}\) 是一个对角矩阵，其中每个对角元素 \(\cos(\theta_{t,k})_r\) 是特征空间中第 \(t\) 和第 \(k\) 个任务之间第 \(r\) 个主角的余弦。 \(\sigma_{t,\cdot}\) 是第 \(t\) 个任务的 \(\cdot\) 的奇异值。 \(\text{RES}_i\) 是需要学习的旋转残差， \(S(i, \cdot)\) 表示直到第 \(i\) 个任务的残差子空间的阶数。 \(|rep|\) 是重放数据的样本数。任务相似性的复杂影响表明了模型架构在协调任务共享和任务特定组件方面的重要性。

此外，找到连续学习理想解决方案的复杂性在很大程度上取决于参数空间的结构。用共享解决方案学习所有增量任务等同于在受限参数空间中学习每个新任务，以防止所有旧任务的性能退化。这样的经典连续学习问题已被证明在一般情况下是NP难的[225]，因为随着更多任务的引入，可行参数空间往

往变得狭窄且不规则，因此难以识别（见图2，e）。这一具有挑战性的问题可以通过重放代表性的旧训练样本[225]，将可行参数空间限制在超矩形[459]，或使用多个连续学习模型替代单个参数空间的模型架构（例如，使用多个连续学习模型）来缓解[96]，[361]，[443]。

为了协调连续学习中的重要因素，最近的工作提出了一种类似的学习和遗忘的广义界形式。例如，在假设所有任务共享具有统一收敛性的全局极小化器（即公式(20)中对于 \(\forall t = 1, ···, k\)，\(\lambda_{\max i} = \lambda\)）下的理想连续学习者[337]，其泛化界为：
$$
c^*_t \leq \mathbb{E}_{D_t \sim D_t} \ell_t(\theta; D_t) \leq c^*_t + \zeta(N_t, \delta), \forall t = 1, ···, k,
$$
其中 \(c^*_t = \ell_t(\theta^*_t; D_t)\) 是第 \(t\) 个任务的最小损失，\(\theta\) 是通过经验风险最小化连续学习 \(1 : k\) 任务的全局解。 \(\zeta = O\left( \frac{\lambda B \sqrt{\log(N_t) \log(|\theta|k/\delta)}}{2\sqrt{N_t}} \right)，并且 \|\theta\|_2 \leq B\)。

考虑到许多不同任务的共享参数空间可能是一个空集（见图2，e），即 \(\bigcup_{t=1}^{k} \theta_t = \emptyset\)，泛化界通过假设 \(K\) 个参数空间（通常 \(K \geq 1\)）来捕获所有任务而进一步细化[442]，[443]。对于新旧任务的泛化误差：
$$
\mathbb{E}_{D_t \sim D_t} \ell_t(\theta; D_t) \leq c^*_t + R\left(\sum_{i=1}^{t-1} \ell^b_i\right) + \sum_{i=1}^{t-1} \text{Div}(D_i, D_t) + \zeta\left(\sum_{i=1}^{t-1} N_i, K/\delta\right),
$$
$$
\sum_{i=1}^{t-1} \mathbb{E}_{D_i \sim D_i} \ell_i(\theta; D_i) \leq \sum_{i=1}^{t-1} c^*_i + R(\ell^b) + \sum_{i=1}^{t-1} \text{Div}(D_t, D_i) + \zeta(N_t, K/\delta)，
$$
其中 \(R(·)\) 和 \(\text{Div}\) 分别是损失平坦性和任务差异的函数。 \(\delta, \theta 和 c^*_t\) 的定义与公式(22) 相同。

这些理论工作表明，理想的连续学习解决方案应提供适当的稳定性-可塑性权衡和足够的任务内/任务间可推广性，这激发了下一节中详细介绍的各种代表性方法。


