--- 
 title: MKGFormer 
 date: 2024-07-05 14:53:42 
 updated: 2024-07-13
 mathjax: true
tags:
  - 持续学习
  - 论文阅读
---

## 综述
### Continual Learning Scenarios

#### Instance-Incremental Learning (IIL)
- **定义**: 所有训练样本属于相同任务，以批次到达。
- **训练**: {Dt,b, t}b∈Bt
- **测试**: {p(Xt)}t=j；不需要任务标识

#### Domain-Incremental Learning (DIL)
- **定义**: 任务具有相同的数据标签空间但不同的输入分布。任务标识不是必须的。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj) 且 Yi = Yj 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；不需要任务标识

#### Task-Incremental Learning (TIL)
- **定义**: 任务具有不相交的数据标签空间。在训练和测试中提供任务标识。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；提供任务标识

#### Class-Incremental Learning (CIL)
- **定义**: 任务具有不相交的数据标签空间。仅在训练中提供任务标识。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；不提供任务标识

#### Task-Free Continual Learning (TFCL)
- **定义**: 任务具有不相交的数据标签空间。在训练和测试中均不提供任务标识。
- **训练**: \{\{Dt,b\}b∈Bt\}t∈T；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；任务标识可选

#### Online Continual Learning (OCL)
- **定义**: 任务具有不相交的数据标签空间。每个任务的训练样本以一次性数据流形式到达。
- **训练**: \{\{Dt,b\}b∈Bt\}t∈T, |b| = 1；p(Xi) ̸= p(Xj) 且 Yi ∩ Yj = ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；任务标识可选

#### Blurred Boundary Continual Learning (BBCL)
- **定义**: 任务边界模糊，具有不同但重叠的数据标签空间。
- **训练**: {Dt, t}t∈T；p(Xi) ̸= p(Xj), Yi ̸= Yj 且 Yi ∩ Yj ̸= ∅ 对于 i ̸= j
- **测试**: {p(Xt)}t∈T；不提供任务标识

#### Continual Pre-training (CPT)
- **定义**: 预训练数据按顺序到达。目标是改进知识传递到下游任务。
- **训练**: {Dpt t , t}t∈T pt , 随后是下游任务 j
- **测试**: {p(Xt)}t=j；不需要任务标识

#### 其他学习场景
除非特别说明，每个任务通常假定有足够数量的标记训练样本，即监督连续学习。根据每个 Dt 中提供的 Xt 和 Yt，连续学习进一步扩展到零样本学习、少样本学习、半监督学习、开放世界学习（识别未知类别并整合其标签）和无监督/自监督学习场景。此外，还考虑并纳入了其他实际挑战，例如多标签、噪声标签、层次粒度和子群体、任务相似性的混合、长尾分布、域对齐、域迁移、随时推理、新类别发现、多模态等。一些最新的工作集中在这些场景的各种组合上，以更好地模拟现实世界的复杂性。

### 连续学习的性能评估

通常，连续学习的性能可以从三个方面进行评估：已经学习任务的总体性能、旧任务的记忆稳定性和新任务的学习可塑性。以下是常见的评估指标，使用分类作为示例。

#### 总体性能
总体性能通常通过平均准确率 (AA) 和平均增量准确率 (AIA) 来评估。

设 \( a_{k,j} \in [0, 1] \) 表示在增量学习第 \( k \) 个任务后，在第 \( j \) 个任务的测试集上评估的分类准确率 (\( j \leq k \))。计算 \( a_{k,j} \) 的输出空间包括 \( Y_j \) 或 \(\bigcup_{i=1}^{k} Y_i\)，分别对应多头评估 (如 TIL) 或单头评估 (如 CIL)。

第 \( k \) 个任务的两个指标定义如下：
$$
AA_k = \frac{1}{k} \sum_{j=1}^{k} a_{k,j}
$$
$$
AIA_k = \frac{1}{k} \sum_{i=1}^{k} AA_i
$$
其中，AA 代表当前时刻的总体性能，AIA 进一步反映了历史性能。

#### 记忆稳定性
记忆稳定性可以通过遗忘度量 (FM) 和向后传递 (BWT) 来评估。

对于前者，任务的遗忘通过其过去的最大性能与当前性能的差值计算：
$$
f_{j,k} = \max_{i \in \{1,...,k-1\}} (a_{i,j} - a_{k,j}), \forall j < k
$$
第 \( k \) 个任务的 FM 是所有旧任务的平均遗忘：
$$
FM_k = \frac{1}{k-1} \sum_{j=1}^{k-1} f_{j,k}
$$
对于后者，BWT 评估学习第 \( k \) 个任务对所有旧任务的平均影响：
$$
BWT_k = \frac{1}{k-1} \sum_{j=1}^{k-1} (a_{k,j} - a_{j,j})
$$
其中，遗忘通常反映为负的 BWT。

#### 学习可塑性
学习可塑性可以通过不易学习度量 (IM) 和向前传递 (FWT) 来评估。

IM 定义为模型学习新任务的能力不足，通过任务的联合训练性能与持续学习性能之间的差异计算：
$$
IM_k = a^*_k - a_{k,k}
$$
其中，\( a^*_k \) 是随机初始化的参考模型在联合训练 \(\bigcup_{j=1}^{k} D_j\) 时第 \( k \) 个任务的分类准确率。相比之下，FWT 评估所有旧任务对当前第 \( k \) 个任务的平均影响：
$$
FWT_k = \frac{1}{k-1} \sum_{j=2}^{k} (a_{j,j} - \tilde{a}_j)
$$
其中，\( \tilde{a}_j \) 是随机初始化的参考模型在第 \( j \) 个任务的 \( D_j \) 上训练的分类准确率。

#### 其他评估指标
请注意，\( a_{k,j} \) 可以根据任务类型适应其他形式，例如对象检测的平均精度 (AP)、语义分割的交并比 (IoU)、图像生成的 Frechet Inception Distance (FID)、强化学习的标准化奖励等，并应相应调整上述评估指标。此外，还有许多其他有用的指标，例如表示遗忘的线性探针、Hessian 矩阵的最大特征值用于损失景观的平坦性、任何时候推理的准确率曲线下面积、存储和计算的开销用于资源效率等。我们建议读者参考其原始论文。
### 理论基础

在本节中，我们总结了关于连续学习的理论工作，涉及稳定性-可塑性权衡和可推广性分析，并将其与各种连续学习方法的动机联系起来。

#### 稳定性-可塑性权衡

根据第2.1节的基本公式，我们考虑一个通用的连续学习设置，其中具有参数 \(\theta \in \mathbb{R}^{|\theta|}\) 的神经网络需要学习 \(k\) 个增量任务。假设每个任务的训练集和测试集遵循相同的分布 \(D_t, t = 1, ..., k\)，其中训练集 \(D_t = \{X_t, Y_t\} = \{(x_{t,n}, y_{t,n})\}_{n=1}^{N_t}\) 包含 \(N_t\) 对数据标签对。目标是学习一个概率模型 \(p(D_{1:k}|\theta) = \prod_{t=1}^{k} p(D_t|\theta)\)（在条件独立假设下），该模型能够在所有任务上表现良好，记为 \(D_{1:k} := \{D_1, ..., D_k\)）。

判别模型的任务相关性能可以表示为 \(\log p(D_t|\theta) = \sum_{n=1}^{N_t} \log p_\theta(y_{t,n}|x_{t,n})\)。连续学习的核心挑战通常来自学习的顺序性质：在从 \(D_k\) 学习第 \(k\) 个任务时，旧的训练集 \(\{D_1, ..., D_{k-1}\}\) 是不可访问的。因此，关键但困难的是在捕获旧任务和新任务的分布时保持平衡，即确保适当的稳定性-可塑性权衡，其中过度的学习可塑性或记忆稳定性可能会大大妨碍对方（见图2，a，b）。

一个直接的想法是通过存储一些旧的训练样本或训练生成模型来近似和恢复旧的数据分布，这在第4.2节中称为基于重放的方法。根据监督学习的学习理论[154]，通过重放更多近似其分布的旧训练样本来提高旧任务的性能，但这会导致潜在的隐私问题和资源开销的线性增长。生成模型的使用也受到额外资源开销、其自身的灾难性遗忘和表达能力的限制。

另一种选择是通过在贝叶斯框架中公式化连续学习来传播旧数据分布。在网络参数的先验 \(p(\theta)\) 基础上，观察第 \(k\) 个任务后的后验可以通过贝叶斯规则计算：
$$
p(\theta|D_{1:k}) \propto p(\theta) \prod_{t=1}^{k} p(D_t|\theta) \propto p(\theta|D_{1:k-1})p(D_k|\theta),
$$
其中，第 \(k-1\) 个任务的后验 \(p(\theta|D_{1:k-1})\) 成为第 \(k\) 个任务的先验，因此仅使用当前训练集 \(D_k\) 就可以计算新的后验 \(p(\theta|D_{1:k})\)。

然而，由于后验通常是不可解的（除非在非常特殊的情况下），常见的选择是用 \(q_{k-1}(\theta) \approx p(\theta|D_{1:k-1})\) 来近似，同样对于 \(q_k(\theta) \approx p(\theta|D_{1:k})\)。接下来，我们将介绍两种广泛使用的近似策略：

第一种是在线拉普拉斯近似，它使用局部梯度信息将 \(p(\theta|D_{1:k-1})\) 近似为多元高斯[177]，[202]，[222]，[371]，[441]。具体来说，我们可以用 \(\phi_{k-1}\) 参数化 \(q_{k-1}(\theta)\) 并通过在 \(p(\theta|D_{1:k-1})\) 的模态 \(\mu_{k-1} \in \mathbb{R}^{|\theta|}\) 周围执行二阶泰勒展开构建一个近似高斯后验 \(q_{k-1}(\theta) := q(\theta; \phi_{k-1}) = \mathcal{N}(\theta; \mu_{k-1}, \Lambda_{k-1}^{-1})\)，其中 \(\Lambda_{k-1}\) 表示精度矩阵，\(\phi_{k-1} = \{\mu_{k-1}, \Lambda_{k-1}\}\)，同样对于 \(q(\theta; \phi_k), \mu_k 和 \Lambda_k\)。

根据公式(8)，学习当前第 \(k\) 个任务的后验模态可以计算为：
$$
\mu_k = \arg \max_\theta \log p(\theta|D_{1:k}) \approx \arg \max_\theta \log p(D_k|\theta) + \log q(\theta; \phi_{k-1}) = \arg \max_\theta \log p(D_k|\theta) - \frac{1}{2} (\theta - \mu_{k-1})^\top \Lambda_{k-1}(\theta - \mu_{k-1}),
$$
该更新递归自 \(\mu_{k-1} 和 \Lambda_{k-1}\)。同时，\(\Lambda_k\) 从 \(\Lambda_{k-1}\) 递归更新：
$$
\Lambda_k = -\nabla^2_\theta \log p(\theta|D_{1:k})|_{\theta=\mu_k} \approx -\nabla^2_\theta \log p(D_k|\theta)|_{\theta=\mu_k} + \Lambda_{k-1},
$$
其中右侧的第一项是负对数似然的海森矩阵，记为 \(H(D_k, \mu_k)\)。实际上，由于 \(\mathbb{R}^{|\theta|}\) 的高维性，\(H(D_k, \mu_k)\) 通常计算效率低，并且没有保证近似的 \(\Lambda_k\) 对于高斯假设是半正定的。为了解决这些问题，海森矩阵通常用费舍尔信息矩阵（FIM）来近似：
$$
F_k = \mathbb{E}[\nabla_\theta \log p(D_k|\theta) \nabla_\theta \log p(D_k|\theta)^\top]|_{\theta=\mu_k} \approx H(D_k, \mu_k)。
$$
为了便于计算，FIM 可以进一步简化为对角近似[177]，[222] 或克罗内克分解近似[293]，[371]。然后，公式(9) 通过保存旧模型 \(\mu_{k-1}\) 的冻结副本来实现，以规避参数变化，称为第4.1节中的基于正则化的方法。这里我们以 EWC [222] 为例，给出其损失函数：
$$
L_{EWC}(\theta) = \ell_k(\theta) + \frac{\lambda}{2} (\theta - \mu_{k-1})^\top \hat{F}_{1:k-1}(\theta - \mu_{k-1}),
$$
其中 \(\ell_k\) 表示特定任务的损失，FIM \(\hat{F}_{1:k-1} = \sum_{t=1}^{k-1} \text{diag}(F_t)\) 使用每个 \(F_t\) 的对角近似 \(\text{diag}(\cdot)\)，\(\lambda\) 是控制正则化强度的超参数。

第二种是在线变分推理（VI）[7]，[91]，[206]，[235]，[248]，[280]，[318]，[378]，[410]。有很多不同的方法来实现这一点，其中一个代表性的方法是最小化当前第 \(k\) 个任务在满足 \(p(\theta|D_{1:k}) \in Q\) 的家族 Q 上的以下 KL 散度：
$$
q_k(\theta) = \arg \min_{q \in Q} KL(q(\theta) \parallel \frac{1}{Z_k} q_{k-1}(\theta)p(D_k|\theta)),
$$
其中 \(Z_k\) 是 \(q_{k-1}(\theta)p(D_k|\theta)\) 的归一化常数。实际上，上述最小化可以通过使用附加的蒙特卡罗近似来实现，并指定 \(q_k(\theta) := q(\theta; \phi_k) = \mathcal{N}(\theta; \mu_k, \Lambda_k^{-1})\) 为多元高斯。这里我们以 VCL [318] 为例，它最小化以下目标（即最大化其负值）：
$$
L_{VCL}(q_k(\theta)) = \mathbb{E}_{q_k(\theta)}(\ell_k(\theta)) + KL(q_k(\theta) \parallel q_{k-1}(\theta))，
$$
其中 KL 散度可以以闭式形式计算，并作为隐含的正则化项。特别地，尽管公式(12) 和公式(14) 的损失函数形式相似，前者是基于一组确定参数 \(\theta\) 的局部近似，而后者是通过从变分分布 \(q_k(\theta)\) 采样计算的。这归因于两种近似策略之间的基本差异[318]，[420]，在特定设置中表现略有不同。

除了参数空间，顺序贝叶斯推理的思想也适用于函数空间[326]，[378]，[417]，这往往能够在更新参数时提供更多的灵活性。此外，还有许多 VI 的其他扩展，例如通过变分自回归高斯过程（VAR-GPs）改善后验更新[206]，构建任务特定参数[7]，[232]，[244]，[280]，以及适应非平稳数据流[235]。

本质上，无论是重放还是正则化，连续学习的约束最终都体现在梯度方向上。因此，一些最新的工作直接操纵基于梯度的优化过程，归类为第4.3节中的基于优化的方法。具体来说，当在内存缓冲区中保留任务 \(t\) 的一些旧训练样本 \(M_t\) 时，鼓励新训练样本的梯度方向与 \(M_t\) 的梯度方向保持接近[66]，[281]，[411]。这被公式化为
$$
\langle\nabla_\theta L_k(\theta; D_k), \nabla_\theta L_k(\theta; M_t)\rangle \geq 0 \quad \text{for} \; t \in \{1, ..., k-1\},
$$
以实质上强制旧任务的损失不增加，即 \(L_k(\theta; M_t) \leq L_k(\theta_{k-1}; M_t)\)，其中 \(\theta_{k-1}\) 是学习第 \(k-1\) 个任务结束时的网络参数。

或者，梯度投影也可以在不存储旧训练样本的情况下进行[65]，[115]，[146]，[202]，[227]，[258]，[266]，[333]，[380]，[448]，[496]。这里我们以 NCL [202] 为例，它在在线拉普拉斯近似中使用 \(\mu_{k-1}\) 和 \(\Lambda_{k-1}\) 操作梯度方向。如公式(15)所示，NCL 通过在以 \(\theta\) 为中心的半径 \(r\) 区域内的距离度量 \(d(\theta, \theta + \delta) = \sqrt{\delta^\top \Lambda_{k-1} \delta / 2}\) 进行最小化任务特定损失 \(\ell_k(\theta)\) 来执行连续学习，该距离度量考虑了先验的曲率通过其精度矩阵 \(\Lambda_{k-1}\)：
$$
\delta^* = \arg \min_\delta \ell_k(\theta + \delta) \approx \arg \min_\delta \ell_k(\theta) + \nabla_\theta \ell_k(\theta)^\




