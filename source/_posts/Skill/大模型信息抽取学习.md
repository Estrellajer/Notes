---
title: 大模型信息抽取学习
mathjax: false
tags:
  - 大模型
  - 信息抽取
categories:
  - Skill
date: 2024-07-15 16:59:17
---

### 大模型相关知识
#### 什么是大模型
###### 大小模型的区别
涌现能力（大模型由于scaling law才具有的能力）
涌现能力是否存在——用户的体验本身就是离散的（选择已经能用的，而不是不能用的里面错误少的）
##### 大模型和预训练语言模型任务的区别
语言建模（PLM）——复杂任务求解（LLM）

![image-20240723104401035](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407231044145.png)
#### 大模型训练相关
##### 数据的准备
##### 如何进行数据的清洗：
- 质量过滤：先启发式规则过滤，然后分类器级联
- 敏感内容过滤：基于规则
- 数据去重：计算粒度；匹配去重

高质量数据对模型训练至关重要。Phi-1模型在高质量数据上训练，达到了HumanEval测试50.6%的pass@1准确率，而低质量数据会导致训练不稳定。GLaM模型的对比研究表明，高质量数据显著提升了自然语言处理任务的表现。预训练数据的准确性和多样性对于减少模型输出错误至关重要。例如，“灯泡是爱迪生发明的”是常见误解，训练模型需避免这种错误数据，以提升模型的准确性和基础能力。如果模型在包含事实性错误或过时的数据上进行训练，会在处理相关主题时产生不准确或虚假的信息，这种现象被称为“幻象”。

##### 如何对语料进行分词
##### BPE算法详解与示例
**背景**：BPE（Byte Pair Encoding）算法最早在1994年被提出用于通用数据压缩，后来被引入自然语言处理领域用于文本分词。BPE算法从一组基本符号（例如字母和边界字符）开始，迭代地寻找语料库中两个相邻词元，将它们合并为新的词元，过程一直持续到预定义的词表大小为止。
##### BPE算法实现步骤
1. **初始化词汇表**： 开始时，词汇表由所有单独的字母和一个边界字符组成。
2. **计算词频**： 统计每个单词在语料库中的频率。
3. **计算字符对频率**： 计算所有相邻字符对在词频字典中的共现频率。
4. **合并字符对**： 每次迭代选择频率最高的字符对进行合并，并更新词汇表和词频字典。
##### BPE算法示例
假设语料库包含以下五个单词及其频率：
```
“loop”：15次  
“pool”：10次  
“loot”：10次  
“tool”：5次  
“loots”：8次
```
初始词汇表为：
```
["l", "o", "p", "t", "s"]
```
###### 初始状态
1. **词频统计**：
    "loop"：15次  
    "pool"：10次  
    "loot"：10次  
    "tool"：5次  
    "loots"：8次
2. **初始词频字典**：
    {"l o o p </w>": 15, "p o o l </w>": 10, "l o o t </w>": 10, "t o o l </w>": 5, "l o o t s </w>": 8}
###### 第一次迭代
1. **计算字符对频率**：
    ("l", "o")：33次  
    ("o", "o")：48次  
    ("o", "p")：15次  
    ("p", "l")：10次  
    ("o", "t")：18次  
    ("t", "s")：8次
2. **合并频率最高的字符对**： 合并 ("o", "o")，新词汇表和词频字典为：
    词汇表：["l", "o", "p", "t", "s", "oo"]  
    词频字典：{"l oo p </w>": 15, "p oo l </w>": 10, "l oo t </w>": 10, "t oo l </w>": 5, "l oo t s </w>": 8}
###### 第二次迭代
1. **计算新字符对频率**：
    ("l", "oo")：33次  
    ("oo", "p")：15次  
    ("p", "oo")：10次  
    ("oo", "l")：10次  
    ("oo", "t")：18次  
    ("t", "s")：8次
2. **合并频率最高的字符对**： 合并 ("l", "oo")，新词汇表和词频字典为：
    词汇表：["l", "o", "p", "t", "s", "oo", "loo"]  
    词频字典：{"loo p </w>": 15, "p oo l </w>": 10, "loo t </w>": 10, "t oo l </w>": 5, "loo t s </w>": 8}
##### Tips
为了训练出高效的分词器，需重点关注以下因素：
1. **无损重构**：确保分词结果能准确重构为原始输入文本。
2. **高压缩率**：在分词后生成的词元数量尽可能少，提高编码和存储效率。

此外，针对特定需求设计分词器，如处理多语言数据或提升特定能力（如数学能力），可以显著提高大语言模型在实际应用中的效果。综上所述，在设计和训练分词器时，需综合考虑这些因素，以确保其在实际应用中发挥最佳效果。
#### 预训练数据准备流程概述：以YuLan模型为例
在大语言模型预训练过程中，数据的准备是至关重要的一步。以下是YuLan模型在预训练阶段的一般流程和关键要点：
##### 1. 数据收集
**目标**：确保预训练数据来源的多样化，以增强模型的广泛适应能力和特定任务能力。
**步骤**：
- **多样化数据来源**：收集大规模网页数据（如Common Crawl）、书籍数据（如Books3和Gutenberg）、高质量知识密集型语料（如知乎和维基百科）。
- **特定任务数据**：根据需求引入特定任务的数据，如数学数据（Proof-Pile）和代码数（GitHub），以优化模型的特定能力。
**YuLan模型的实践**：
- 首先收集了大量通用预训练语料，包括网页数据和书籍数据。
- 为增加数据多样性，收集了知乎、维基百科等高质量语料。
- 在训练后期，引入了数学和代码等专用文本数据，以增强特定任务能力。
#### 2. 数据清洗
**目标**：提高数据质量，确保模型在预训练过程中能够学到有用的信息。
**步骤**：
- **通用数据清洗**：进行质量过滤、去重、隐私数据去除和词元化。
- **针对性清洗**：根据具体数据特点和应用场景设计清洗规则，例如过滤掉网页数据中的HTML标签。
**YuLan模型的实践**：
- 采用启发式方法进行文档级别和句子级别的低质量及有害数据过滤。
- 使用MinHash算法在多个数据源之间识别并去除重复数据。
- 基于LLaMA的词表，并加入在中文预训练数据上得到的BPE词元，构成YuLan模型的词表（词表大小为51,200），用于对预训练数据进行词元化。
#### 3. 数据调度
**目标**：确定训练大语言模型的数据混合配比及数据训练顺序，以优化模型能力。
**步骤**：
- **代理方法**：使用多个候选策略训练多个小型语言模型，从中选择最优的训练策略。
- **确定混合比例**：首先确定语言配比，再确定不同数据类型配比，通过下游任务测试效果来调整比例。
- **动态调整**：根据模型各项能力的测试结果，动态调整数据混合比例。
**YuLan模型的实践**：
- 预训练了一个1.3B的小模型，测试不同类型数据和中英文数据的混合配比。
- 根据下游任务测试效果确定中英文语料比例为1:8。
- 维持该比例不变，并使用控制变量法，每次仅调整某一类型数据的比例进行实验。
- 根据各项能力的测试结果手动调整数据混合比例，最终使用了1,680B词元，包括1,380B英文数据、280B中文数据以及20B多语数据。
#### 数据课程
**目标**：通过特定顺序安排预训练数据，优化模型的学习效果。
**步骤**：
- 从简单/通用数据开始，逐渐引入更具挑战性/专业化的数据。
- 在训练期间动态调整数据混合比例。
**YuLan模型的实践**：
- 例如，为提升代码生成能力，首先使用2T通用词元进行训练，然后用500B代码数据词元进行训练。
- 为提升数学能力，选择CodeLLaMA作为基座模型，在包含科学论文、数学和代码的混合数据集合上进行继续预训练。
#### 总结
YuLan模型的预训练数据准备流程包括数据收集、数据清洗和数据调度三个主要步骤。在数据收集中注重多样化和特定任务数据的引入；在数据清洗中进行全面细致的质量过滤、去重和词元化；在数据调度中，通过小模型测试和动态调整确定最优的数据混合配比和训练顺序。通过这些步骤，确保了YuLan模型在预训练阶段能够获得高质量的数据支持，从而提升模型的各项能力。

### 位置编码
#### 旋转矩阵计算
假设我们有一个简单的输入序列：“Hello”, “world”, “AI”。假设词向量的维度为 (H = 4)。
##### 步骤 1: 生成词向量

**词嵌入（Word Embedding）**:
- ( \text{Hello} \rightarrow [1.0, 2.0, 3.0, 4.0] )
- ( \text{world} \rightarrow [2.0, 3.0, 4.0, 5.0] )
- ( \text{AI} \rightarrow [3.0, 4.0, 5.0, 6.0] )
##### 步骤 2: 旋转矩阵的计算
**旋转矩阵 ( R_{\theta, t} )**:
- 基数 ( b = 10000 )
- 旋转角度 ( \theta_1 = 0.01 ) 弧度
对每个位置 ( t ):

$$  
R_{\theta, 0} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}  
$$

$$  
R_{\theta, 1} = \begin{bmatrix} \cos(0.01) & -\sin(0.01) \\ \sin(0.01) & \cos(0.01) \end{bmatrix} \approx \begin{bmatrix} 0.99995 & -0.00999983 \\ 0.00999983 & 0.99995 \end{bmatrix}  
$$
$$  
R_{\theta, 2} = \begin{bmatrix} \cos(0.02) & -\sin(0.02) \\ \sin(0.02) & \cos(0.02) \end{bmatrix} \approx \begin{bmatrix} 0.9998 & -0.0199987 \\ 0.0199987 & 0.9998 \end{bmatrix}  
$$
##### 步骤 3: 应用旋转矩阵到词向量
**应用旋转矩阵**:
$$  
\text{Hello\_rot} = [1.0, 2.0, 3.0, 4.0]  
$$

$$  
\text{world\_rot} = [1.9998, 2.98002, 3.9994, 4.9801]  
$$

$$  
\text{AI\_rot} = [2.9994, 3.95998, 4.9988, 5.9596]  
$$
##### 总结
旋转位置编码（RoPE）通过应用位置特定的旋转矩阵到每个词向量的维度上，有效地融合了词的语义息和它在句子中的

位置信息。这种方法不仅保留了原始词向量的语义特征，还增加了位置敏感性，从而使模型能够更好地理解和处理序列数据。
### SFT和RLHF对比
![image-20240725094521712](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407250945768.png)
### ICL和CoT对比
![image-20240725101414135](https://picoflmq.oss-cn-beijing.aliyuncs.com/typora/202407251014182.png)
#### 上下文学习（ICL）和思维链提示（CoT）的关键区别：

##### 上下文学习（In-context Learning, ICL）
- **示例内容**：示例里仅包含问题和最终答案。
- **主要特点**：模型基于任务描述和示例问题-答案对进行学习，并直接生成答案。
- **适用场景**：适用于简单的、示例明确的任务。
##### 思维链提示（Chain-of-Thought, CoT）
- **示例内容**：示例里不仅包含问题和答案，还包括详细的推理过程。
- **主要特点**：通过展示每一步的推理过程，帮助模型理解和解决复杂任务。
- **适用场景**：适用于需要多步骤推理的复杂任务，增强模型的推理能力。
##### 具体例子对比：
###### 上下文学习（ICL）示例：
任务描述: Answer the following mathematical reasoning questions:  

示例1:  
Q: If you have 12 candies and you give 4 candies to your friend, how many candies do you have left?  
A: The answer is 8.  
​
示例2:  
Q: If a rectangle has a length of 6 cm and a width of 3 cm, what is the perimeter of the rectangle?  
A: The answer is 18cm.  
​  
测试样本:  
Q: Sam has 12 marbles. He gives 1/4 of them to his sister. How many marbles does Sam have left?

模型基于示例直接生成答案：

A: The answer is 9.
###### 思维链提示（CoT）示例：
任务描述: Answer the following mathematical reasoning questions:  
​ 
示例1:  
Q: If a rectangle has a length of 6 cm and a width of 3 cm, what is the perimeter of the rectangle?  
A: For a rectangle, you add up the length and width and double it. So, the perimeter of this rectangle is (6 + 3) x 2 = 18 cm. The answer is 18cm.  
​  
测试样本:  
Q: Sam has 12 marbles. He gives 1/4 of them to his sister. How many marbles does Sam have left?

模型基于示例中的推理步骤进行详细推理并生成答案：

A: He gives (1/4) x 12 = 3 marbles. So Sam is left with 12 - 3 = 9 marbles. The answer is 9.
#### 总结：
- **ICL**：示例中仅有问题和答案，适用于简单任务。
- **CoT**：示例中包含详细推理过程，适用于复杂任务，需要多个推理步骤。


### retrieval aug
### Prompt IE
#### 1. 问答（Question Answer，QA）
LLMs通常通过对话方法进行指令微调，这与IE任务的结构化预测要求存在差距。因此，最近的一些研究尝试使用QA提示方法来增强LLMs，并促进生成所需的结果。例如：
- **QA4RE** 发现LLMs在RE任务上表现不佳，因为用于训练它们的指令微调数据集中RE任务的出现频率较低。因此，它提出将RE重新表述为多选QA，以利用指令微调数据集中QA任务的高出现率。
- **Li等人** 分析了现有RE提示的局限性，提出了一种新的方法，称为“总结和提问”（summarize-and-ask）提示，通过递归地使用LLMs将零样本RE输入转换为有效的QA格式。
- **ChatIE** 提出了一种两阶段框架，将零样本IE任务转换为多轮QA问题。该框架首先识别不同类型的元素，然后针对每种元素类型执行顺序IE过程。

#### 2. 思维链提示（Chain-of-Thought，CoT）
思维链（CoT）是一种通过提供逐步和连贯的推理链作为提示来指导模型生成响应的策略。这种提示在近年来受到了关注，并且有研究正在探索其在IE任务中的有效性。例如：
- **PromptNER** 结合LLMs和基于提示的启发式方法及实体定义，促使LLMs根据提供的实体类型定义生成潜在实体及其解释的列表。
- **Bian等人** 提出了一种两步法，通过使用CoT逐步解决生物医学NER任务，首先进行实体范围提取，然后确定实体类型。
- **Yuan等人** 提出了CoT提示作为一种两阶段方法，指导ChatGPT执行时间关系推理任务。

###   Prompt设计

请根据以下示例和格式要求，从给定的航空通告（NOTAM）文本中提取相关信息：

示例1： 输入： D) MAR 16 17 20-31 APR 01-15 0030-1030 E)DUE TO UNMANNED ACFT, ONLY USE ATS RTE Q14 FOR FLT OPERATIONS EQUAL TO, OR ABOVE FL100 (SEE NOTAM VVVV A0355/24) RMK: PILOTS ARE REQ TO FLW ATC INSTRUCTIONS STRICTLY.

输出： { "type": "禁航", "atc": null, "fpl": null, "height_detail": { "upper": "UNL", "lower": "FL100" }, "area_id": null, "change_info": null, "directional": null, "route": "Q14", "start": null, "end": null, "exclude_info": null }

示例2： 输入： D) E)ATS RTE SEGMENTS CLSD: A100 KRASNYY SULIN NDB(KL)- ARNAD A102 BABUR-NALEM A225 GUKOL-TIKNA

输出： { "type": "禁航", "atc": null, "fpl": null, "height_detail": null, "area_id": null, "change_info": null, "directional": null, "route": ["A100", "A102", "A225"], "start": ["KL", "BABUR", "GUKOL"], "end": ["ARNAD", "NALEM", "TIKNA"], "exclude_info": null }

请注意：

1. 输出应为JSON格式。
    
2. 如果某个字段在给定文本中没有相关信息，请将其值设为null。
    
3. 对于route、start和end字段，如果有多个值，请使用数组格式。
    
4. 请仔细分析文本，提取所有可能的相关信息。
    
5. 如果文本中包含多条信息，请尽可能提取所有相关内容。
    

现在，请根据以上要求和示例，从下面给出的NOTAM文本中提取相关信息：

D)19-22 0430-1700 E)ATS RTE SEGMENT CLSD: KR395 LYSKOVO NDB(HD)-OSVUP.